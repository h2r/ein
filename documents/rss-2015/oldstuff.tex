
\subsection{PID Parameter Training}












\subsection{Object Grasping}
\jgonote{Including open and closed loop paradigms, learning specific and generic grasp models.}

We consider a setting where a robotic arm
with 7 degrees of freedom grasps objects with a parallel plate gripper which adds
an additional degree of freedom, but much of what we discuss could be extended to other
arms and grippers, for instance the universal jamming gripper ~\citep{}.

We use a dual rate PID controller in the sense that we use two sets of PID coefficients. The
first set is for making large adjustments when the aim if off by a significant amount. The
second set is for making small adjustments when aim is close to the target.

Our system is distributed and thus at times there is an appreciable amount of latency between
communicating components. Care is taken to synchronize robot movement with object detection
reports, allowing only a fixed amount of movement per report.

Visual servoing tutorial paper: \citep{chaumette06}

\begin{figure*}
  \begin{center}
    \begin{tabular}{l c}
      \includegraphics[width=200px, height=150px]{robo2.png} &
      \includegraphics[width=200px, height=150px]{screen2.png} \\
    \end{tabular}
  \end{center}
  \caption{Left: Baxter uses visual servoing to grasp an object. Right: The view through Baxter's wrist camera, 
    showing the location of the target as well as the objective reticle.}
\end{figure*}

\stnote{Cite the visual servoing tutorial paper and talk about the
  connection to grasping rectangles.}

This Grasp Rectangle business fits in nicely with the reticle.

0. Estimate the depth of the table by inspecting non-object locations. This helps decide when to close the gripper.

1. Servo orientation to the '0' orientation, or one of a few sparsely sampled keypoint orientations.
Each viewing orientation (from the wrist) is tied to a grasping orientation.

2. Servo to a 'normal' scale. This is fixed, we don't want multiple scales running around.

3. Now instead of aiming at the center, aim at a proposed target offset from the center.


\subsection{PID Controller}
\jgonote{Maybe a coordinate descent algorithm or wide-scale random noise search.}
Since we use a dual-rate controller, there are two separate sets of coefficients that we must train.
We train the high-rate coefficients with the objective of getting the aim within the ``close'' threshold.
We train the low-rate coefficients with the objective of getting the aim within the 'hit' threshold.
The training for the high and low-rate coefficients is analogous and happens independently, so
without loss of generality we describe the training process for an arbitrary set of coefficients.

\jgonote{I imagine this has been done before so it would be good to find who did this.}
\stnote{Find someone to learn PID coefficients.}
A single set of PID coefficients consists of $K_P$, $K_I$, $K_D$. In the inside loop of EM-like training, we
randomly pick a coefficient $K$ to train, fix the other two coefficients,  and use a local search 
algorithm ~\citep{} to find the optimal value of $K$ conditioned on the fixed values of the other two
parameters. This problem is not necessarily convex and so we run the inside loop of our algorithm
until we have converged to a local minimum.


\section{Simultaneous Localization and Mapping of Tabletop Objects}

We aim to autonomously acquire instance-based models of objects based
on exploration.  To carry out this inference, we follow approaches
based on recursive state estimation~\citep{thrun08} and
SLAM~\citep{durrant06}.  In our approach, the map that the robot is
acquiring is not an occupancy grid or landmark locations, as in
conventional SLAM, but rather knowledge of the appearance and grasp
points of the objects being mapped.  The localization problem is
estimating the pose of these objects at each time step.  We can also
jointly estimate the pose of the robot's end effector, but this pose
is often known with high accuracy in the local coordinate system.  The
graphical model for our approach appears in Figure~\ref{fig:model}.

\begin{figure}
\centering
\includegraphics{model-crop.pdf}
\caption{Graphical model for our proposed approach.\label{fig:model}}
\end{figure}

We define the following variables:
\begin{itemize}
\item $x_t = \left< r_t, o_1 \dots o_k \right>$.  Our state consists of
  the pose of the robot's end effector, $r_t$, along with the pose and
  category of all the objects currently being tracked, $o_1 \dots o_k$.
\item $z_t = \left<r, g, b, d\right>$.  Our observation consists of
  the pixel values and depth generated from a sensor on the robot's
  end effector.  This sensor could be a depth camera from the Kinect;
  in our experiments we use a one-pixel depth camera created from
  Baxter's IR sensor and wrist camera.
\item $u_t = \left\{\left<x, y, z, r, p, y\right>\right.$ Alternately,
  the robot could attempt to grasp the object, leading to a new
  configuration of objects in the workspace.
\item $m$ The map consists of a colored voxel map for each object. 
\end{itemize}

We aim to jointly estimate a distribution over object poses along with
the map, $m$:
\begin{align}
p(x_t, m |  z_0 \dots z_t, u_0 \dots u_t)
\end{align}

We formulate this problem as a Bayes' filter with a time update:
\begin{align}
p&(x_t, m | z_0 \dots z_{t-1}, u_0 \dots u_t) = \notag\\
&\int p(x_t | x_{t-1}, u_t) \times p(x_{t-1}, m | z_{0} \dots z_{t-1}, u_{0} \dots u_{t-1}) dx_{t-1}
\end{align}

and a measurement update: 
\begin{align}
p(&x_t, m | z_0 \dots z_t, u_0 \dots u_t) = \notag\\
&\frac{p(z_t | x_t, m) P(x_t, m | z_0 \dots z_{t-1}, u_0 \dots u_t)}
{p(z_t | z_{0} \dots z_{t-1}, u_0 \dots u_t)}
\end{align}

\subsection{Transition Model}

Our transition model incorporates the effect of the control input on
the robot's end effector as well as on all the objects:
\begin{align}
p(x_t | x_{t-1}, u_t) &= p(r_t, o^1_t \dots o^k_t | r_{t-1}, o^1_{t-1} \dots o^k_{t-1}, u_t) \\
\intertext{We assume that the robot's end effector pose and the object poses are independent:}
                     &= p(r_t | u_t) \times p(o^1_t \dots o^k_t | o^1_{t-1} \dots o^k_{t-1}, u_t)
\intertext{
We also assume that each object is independent of the
others; although in general we would like the robot to predict the
effects of its grasps, in practice other objects can be moved by
mistake.  Therefore our transition model will have high uncertainty
over all objects when the robot moves any object.}
                    &\approx p(r_t | u_t) \times \prod_k p(o^k_t | o^k_{t-1}, r_t)
\end{align}

We endow the robot with two types of control actions.  In the first
type, the robot plans collision free motions with conservative models
of the boundaries of possible objects; these types of transitions move
the robot's end effector but leave all the object poses unchanged.
The second type is when the robot grasps an object.  After an
attempted grasp we assume the pose of all the objects have moved and
need to be reestimated (in case the robot knocked an object during its
attempted grasp)

\subsection{Measurement Model}

Our sensor consists of a depth camera; for simplicity we update one
pixel at a time, although a sensor such as the Kinect would consist of
many updates at once.
\begin{align}
p(z_t | x_t, m) = p(z_t | r_t, o_t^1 \dots o_t^k, m)
%
\intertext{When the object pose and map is known, we can identify the
  one object penetrated by the sensor:}
%
p(z_t | x_t, m) = p(z_t | r_t, o_t^i, m^i)
\end{align}

We assume a sensor model with Gaussian noise on pixel values and depth
values.

\subsection{Inference}

We carry out inference in the model using a Rao-Blackwellized Particle
Filter~\citep{durrant06}.  In particular we partition our state space
according to the product rule:

\begin{align}
p(&x_0 \dots x_t, m | z_0 \dots z_t, u_0 \dots u_t) = \notag\\
& p(m | x_0 \dots x_t, z_0 \dots z_t) \times p(x_0 \dots x_t | z_0 \dots z_t, u_0 \dots u_t)
\end{align}

Thus each particle is represented by the set:
\begin{align}
\left\{ w_t^{(i)}, x_0^{(i)} \dots x_t^{(i)}, p(m | x_0^{(i)} \dots x_t^{(i)}, z_0 \dots z_t)\right\}
\end{align}

In a simplified model, we can compute a maximum likelihood map
analytically from the pose and observation history, rather than
representing a distribution over it.

Our proposal distribution is the motion model, as in Fast Slam
1.0~\citep{montemerlo02}:
\begin{align}
x_t^{(i)} \sim  p(x_t | x_{t-1}^{(i)}, u_t)
\end{align}

That is, when objects do not move, we assume a fixed pose, together
with small random movement to correct initial pose estimation error.
