\documentclass[conference]{IEEEtran}
\usepackage{times}
\usepackage{booktabs}
\usepackage{latexsym,amssymb,amsmath} % for \Box, \mathbb, split, etc.
% \usepackage[]{showkeys} % shows label names
\usepackage{cite} % sorts citation numbers appropriately
\usepackage{path}
\usepackage{url}
\usepackage{verbatim}
\usepackage[pdftex]{graphicx}
\usepackage[numbers]{natbib}
\usepackage{amsfonts, amssymb, amsmath}
\usepackage[usenames,dvipsnames]{color}

\newcommand{\stnote}[1]{\textcolor{Blue}{\textbf{ST: #1}}}
\newcommand{\jgonote}[1]{\textcolor{Green}{\textbf{JGO: #1}}}

\usepackage{algorithmic}

\newcommand{\mytitle}[0]{Autonomously Acquiring Models of Objects for
  Instance-Based Grasping}

\pdfinfo{
   /Author ()
   /Title  (\mytitle)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords ()
}



\makeatletter
\setlength{\arraycolsep}{2\p@} % make spaces around "=" in eqnarray smaller
\makeatother


% begin of personal macros
\newcommand{\half}{{\textstyle \frac{1}{2}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\myth}{\vartheta}
\newcommand{\myphi}{\varphi}

\newcommand{\IN}{\mathbb{N}}
\newcommand{\IZ}{\mathbb{Z}}
\newcommand{\IQ}{\mathbb{Q}}
\newcommand{\IR}{\mathbb{R}}
\newcommand{\IC}{\mathbb{C}}
\newcommand{\Real}[1]{\mathrm{Re}\left({#1}\right)}
\newcommand{\Imag}[1]{\mathrm{Im}\left({#1}\right)}

\newcommand{\norm}[2]{\|{#1}\|_{{}_{#2}}}
\newcommand{\abs}[1]{\left|{#1}\right|}
\newcommand{\ip}[2]{\left\langle {#1}, {#2} \right\rangle}
\newcommand{\der}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\dder}[2]{\frac{\partial^2 {#1}}{\partial {#2}^2}}

\newcommand{\nn}{\mathbf{n}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\uu}{\mathbf{u}}

\newcommand{\junk}[1]{{}}

% set two lengths for the includegraphics commands used to import the plots:
\newlength{\fwtwo} \setlength{\fwtwo}{0.45\textwidth}


\renewcommand{\labelitemi}{}
\renewcommand{\labelitemii}{}
\renewcommand{\labelitemiii}{}


% end of personal macros
% \input{inputFile.tex}


\begin{document}
\DeclareGraphicsExtensions{.jpg}


\title{\mytitle{}}

\author{Author Names Omitted for Anonymous Review. Paper-ID [add your ID here]}

\maketitle


\begin{abstract}
Manipulating objects is an important task for robots that help people
in the home, in factories, and in hospitals.  General-purpose
pick-and-place requires object recognition, pose estimation, and grasp
planning; existing solutions cannot reliably recognize or pick up an
object the robot has never encountered before~\citep{}.  However in
many applications, general-purpose pick-and-place is not required: the
robot would be useful if it could recognize and manipulate the small
set of objects most important in that application, but do so with high
reliability.  To address this issue, we propose an architecture for
perception and actuation that enables a robot to quickly and easily
acquire instance-level descriptions of novel objects.  The robot
learns to recognize, estimate the pose, and grasp the object through
active data collection as well as by asking a human annotator for
specific supervision.  Our approach converts the task of {\em
  category recognition} (pick up any mug) to {\em instance
  recognition} (pick up this mug), a much easier problem that conventional
methods can solve with the right training data.  By leveraging
this first-hand experience with the object, naive users can
quickly train our system to reliably localize and grasp the specific
objects that they care about for their particular application.
As a byproduct, our approach collects data on grasping and
manipulation that we can use to train general-purpose models. Using
our approach, a robot can interact with an object for ten minutes, and
then reliably localize (90\%) and manipulate it (90\% successful
grasps).


%% Robots need to pick stuff up.
%% %
%% Perception can structure data, structured data facilitates planning, planning allows more principled perception.
%% %
%% The tasks of robotic perception, planning, and control will all benefit from
%% a design paradigm which allows them to be jointly programmed.
%% %
%% So we contribute an implementation \emph{NODE} which facilitates the design and 
%% execution of versatile and scalable MDPs structured in what we call a
%% Von Neumann MDP pattern. 
%% %
%% We form a closed loop through perception and planning, allowing them to interact by means of structured data.
%% %
%% Our system incorporates online, human-in-the-loop algorithms. Non-technical human participants can easily
%% teach and collaborate with the system.
\end{abstract}


% First paragraph: What is the problem we are trying to solve? I
% think it's something about enabling a robot to robustly perceive and
% manipulate native objects, so that it can carry out tasks such as
% assisting at childcare, cooking, or in hospitals.
% Second paragraph: Why hasn't previous work addressed this problem?
% I think it's something about the focus on category recognition, not
% doing pose estimation, and training methods that require an expert
% user and an annotated corpus.
% Third paragraph: What do we propose to do to address this problem?
% Fourth paragraph: Our high-level technical approach
% Fifth paragraph: How do we know it works? Why is it cool?
% Something about an evaluation and something about a robotic
% demonstration.

\section{Introduction}
Robotic assistants will assist us at childcare, help us cook, and provide service to
doctors, nurses, and patients in hospitals. Many such tasks require a robot to
robustly perceive and manipulate objects. 
Conventional systems are capable of perception and manipulation in a limited sense.
Some systems require training by a human operator on an object to object basis, which 
is time consuming and can be difficult for a non-expert to perform ~\citep{ork14, lai11, lai11a}. There 
do exist some systems which
do not require training on a per object basis, but they are computationally expensive and do not enjoy the
highest accuracy or precision and have not been demonstrated for grasping~\citep{guadarrama14}.

To obtain the benefits of both approaches, we propose a system which trains itself 
to recognize and manipulate the specific objects it will need to use during future
collaborations with humans. 
Our system is powerful because it learns to identify and grasp on 
a per object basis. Our system is portable, convenient, and general because the expert knowledge it employs
is built into the algorithms which it uses to train itself, requiring only basic
interaction from a non-technical human collaborator.

Our contribution is an algorithm which allows a robot to autonomously
train its subsystems, together with three applications of the
algorithm to the tasks mentioned above.  The first application is
recognizing the category of an object and the second application is
estimating the pose of the object, both of which we accomplish with
simple and robust computer vision algorithms combined with our
  proposed algorithm for autonomous collection of training data. The
third application is grasping the object, which we accomplish with
visual servoing techniques. Each of these components is well
understood in its own right, and existing methods allow expert users
to train systems to accomplish these tasks satisfactorily.

When we apply the algorithm to the recognition task, the robot trains
the recognition system to discriminate between object instance
categories. When we apply the algorithm to the pose estimation task,
the robot trains the pose estimation system to determine which pose an
identified object holds.  When we apply the algorithm to the grasping
task, the robot trains the grasping system to successfully and quickly
pick and place the target object.  Crucially, our algorithm can
recognize when it is doing a poor job at learning and asks a human
collaborator to manually annotate information in those cases.



It works because our experiments tell us so. This is how well they
work: .  Thus we see an improvement over expert trained systems (cite
usability results), and is an improvement over un-annotated systems
(cite success rates, the ability to generalize, and the low
computational overhead since we don't crunch expensive features or do
heavy classification at run time).

\stnote{We have to say the right things about ORK.  Why does ORK suck?
  Why doesn't it already solve the problem?}


\section{Object Detection and Pose Estimation}

We first describe our instance-based object detection and pose
estimation pipeline, which uses standard computer vision algorithms
combined to achieve a simple software architecture, a high frame rate,
and high accuracy at recognition and pose estimation.  This pipeline
can be manually trained by an expert to reliably detect and grasp
objects.  Additionally, section~\ref{sec:training} describes our
approach to enabling a robot to autonomously train this pipeline by
actively collecting images and training data from the environment.

\subsection{Object Recognition}
\jgonote{Cover BING, SIFT, BoW, typical training pipelines, and RGB-D
  approaches popular in the robotics community.} 

Our recognition pipeline takes RGB-D video from the robot, proposes a
small number of candidate object bounding boxes in each frame, and
classifies each candidate bounding box as belonging to a previously
encountered object class. Our object classes consist of object
instances rather than pure object categories.  Using instance
recognition means we cannot reliably detect categories, such as
``mugs,'' but the system will be able to detect, localize, and grasp
the specific instances for which it has models with much higher speed
and accuracy.

To generate candidate bounding boxes, we first apply the BING
objectness detector~\citep{cheng14} to the image, which returns a set
$\{B_i\}$ of thousands of approximate object bounding boxes in the
image. This process substantially reduces the number of bounding boxes
we need to consider but is still too large for us to process in real
time. Besides, even good bounding boxes from BING are typically not
aligned to the degree that we require. Therefore, we use integral
images to efficiently compute the per-pixel map:
\begin{align}
J(p) = \sum_{B \in \{B_i\} s.t. p \in B} \frac{1}{Area(B)}.
\end{align}
We then apply the Canny edge detector with hysteresis ~\citep{} to find the connected components of bright
regions in the map $J(p)$, which correspond with high probability to objects in the image. We form
our candidate object bounding boxes by taking the smallest bounding box which surrounds each connected component.
These bounding boxes make it easy to gather training data and to perform inference in real time, but at
the expense of poorly handing occulusion as overlapping objects are fused into the same bounding box.
It is possible to search within the proposed bounding boxes to better handle occlusion.

For each object $c$ we wish to classify, we gather a set of example crops $E_c$ which are candidate
bounding boxes (derived as above) which contain $c$. We extract dense SIFT features ~\citep{} from all boxes of
all classes and use k-means to extract a visual vocabulary of SIFT features ~\citep{}. We then construct a
BoW feature vector for each image and augment it with a histogram of colors which appear in that image.
The augmented feature vector is incorporated into a k-nearest-neighbors model which we use to classify
objects at inference ~\citep{}.

The use of SIFT features is motivated by the instance level nature of our task. State-of-the-art vision
methods typically use HOG ~\citep{} or CNN ~\citep{} features, but that choice is motivated by category
level recognition. \stnote{What about category level recognition motivates HOG or CNN?  Can you be more specific?}

We use kNN because it is easy to rebuild online, which is a key property a classifier should enjoy
if it is to interact with our framework in real time. State-of-the-art computer vision classifiers
currently employ SVM's ~\citep{} or other models which require expensive training. Using such a model would
introduce a training step in the inside loop of our data collection process, which would be costly
in either engineering or time.  It is possible to use kNN during the online collection process and then
train a stronger classifier in the background at higher latency, essentially introducing a cascading step
in the data collection process.

\begin{figure*}
  \begin{center}
    \begin{tabular}{l c r}
      \includegraphics[width=160px, height=120px]{kinect.png} &
      \includegraphics[width=160px, height=120px]{objectness.png} &
      \includegraphics[width=160px, height=120px]{blueBoxes.png} \\
    \end{tabular}
  \end{center}
  \caption{The Object Detection Pipeline. Left: The raw image as viewed through the kinect. 
    Center: The computed objectness map J. Right: Labeled object detections. \stnote{Use subfigure}}
\end{figure*}

 
\subsection{Pose Estimation}
\jgonote{Computer vision approaches, geometry and point cloud based approaches. Dieter Fox's automatic
training pipeline (how well developed is it? we may need to sell our approach as being a 
general algorithm for allowing a robot to train arbitrary subsystems in order to differentiate 
ourselves.)}
We tackle the pose estimation problem using the same classification pipeline that we use for
object recognition. We train a separate pose classifier for each object class. This time, the class
assigned to each training example is the orientation from which the object is viewed in that example.
During inference, we first determine the object class of a candidate bounding box, and once the class
is known we apply the corresponding pose classifier to determine the orientation from which we
are viewing the object. We combine this orientation with position information from the point cloud
information derived from the D channel of the RGB-D video to form a full pose estimate.

\subsection{Object Grasping}
\jgonote{Including open and closed loop paradigms, learning specific and generic grasp models.}

We consider a setting where a robotic arm
with 7 degrees of freedom grasps objects with a parallel plate gripper which adds
an additional degree of freedom, but much of what we discuss could be extended to other
arms and grippers, for instance the universal jamming gripper ~\citep{}.

We use a dual rate PID controller in the sense that we use two sets of PID coefficients. The
first set is for making large adjustments when the aim if off by a significant amount. The
second set is for making small adjustments when aim is close to the target.

Our system is distributed and thus at times there is an appreciable amount of latency between
communicating components. Care is taken to synchronize robot movement with object detection
reports, allowing only a fixed amount of movement per report.

Visual servoing tutorial paper: \citep{chaumette06}

\begin{figure*}
  \begin{center}
    \begin{tabular}{l c}
      \includegraphics[width=200px, height=150px]{robo2.png} &
      \includegraphics[width=200px, height=150px]{screen2.png} \\
    \end{tabular}
  \end{center}
  \caption{Left: Baxter uses visual servoing to grasp an object. Right: The view through Baxter's wrist camera, 
    showing the location of the target as well as the objective reticle.}
\end{figure*}

\stnote{Cite the visual servoing tutorial paper and talk about the
  connection to grasping rectangles.}

This Grasp Rectangle business fits in nicely with the reticle.

0. Estimate the depth of the table by inspecting non-object locations. This helps decide when to close the gripper.

1. Servo orientation to the '0' orientation, or one of a few sparsely sampled keypoint orientations.
Each viewing orientation (from the wrist) is tied to a grasping orientation.

2. Servo to a 'normal' scale. This is fixed, we don't want multiple scales running around.

3. Now instead of aiming at the center, aim at a proposed target offset from the center.


\subsection{PID Controller}
\jgonote{Maybe a coordinate descent algorithm or wide-scale random noise search.}
Since we use a dual-rate controller, there are two separate sets of coefficients that we must train.
We train the high-rate coefficients with the objective of getting the aim within the ``close'' threshold.
We train the low-rate coefficients with the objective of getting the aim within the 'hit' threshold.
The training for the high and low-rate coefficients is analogous and happens independently, so
without loss of generality we describe the training process for an arbitrary set of coefficients.

\jgonote{I imagine this has been done before so it would be good to find who did this.}
\stnote{Find someone to learn PID coefficients.}
A single set of PID coefficients consists of $K_P$, $K_I$, $K_D$. In the inside loop of EM-like training, we
randomly pick a coefficient $K$ to train, fix the other two coefficients,  and use a local search 
algorithm ~\citep{} to find the optimal value of $K$ conditioned on the fixed values of the other two
parameters. This problem is not necessarily convex and so we run the inside loop of our algorithm
until we have converged to a local minimum.

\section{General Robotic Autonomous Training Algorithm}
\label{sec:training}
We teach the system by finding its weaknesses and training it to overcome them.

Our framework structures the training in a way that scales. 

When the robot asks for help, we can ground ambiguous object examples to the right class
and retrain the models online.

\subsection{GRATA Definition}

In the following algorithm design pattern, 
$X$ is the set of ground truth labels for the specified problem,
$Y$ is the set of all possible observations for the specified problem,
\mbox{$D \subset X \times Y$} is the set of all possible training pairs observable in our problem instance,
\mbox{$\{T_i\}_{i \in \mathbb{N}}$} is a sequence of curated sets of training examples (where \mbox{$\forall i \in \mathbb{N}, T_i \subset D$}),  
$\mathcal{T}$ is the power set of $D$,
\mbox{$O: \mathcal{T} \to \mathbb{R}$} is the objective function that we are trying to optimize 
(which includes the implementation of the physical actions the robot must take to evaluate it),
\mbox{$S: \mathbb{N} \to \mathbb{R}$} is the objective function history (i.e. \mbox{$S(i) = O(T_i)$}),
$\mathcal{S}$ is the set of all possible objective function histories,
\mbox{$H: \mathcal{S} \times \mathbb{N} \to \{0,1\}$} is a function which decides whether to ask for help at
a specified time step,
\mbox{$\mathcal{C} = \mathcal{T} \times \mathcal{S} \times \mathbb{N}$} is the complete training state,
\mbox{$G: \mathcal{C}$} is a function which implements asking for and receiving help (e.g. auditing and grounding data under bad conditions
or asking for a new collections strategy or a crucial example),
\mbox{$P: \mathcal{C} \to \{0,1\}$} is a function that evaluates a stopping criterion and decides whether to continue training,
and \mbox{$L: \mathcal{C} \to D$} a function that proposes a new ground truth $x \in X$ and collects a corresponding observation $y \in Y$ to form
a new training example \mbox{$(x,y) = d \in D$}. 

\begin{table}
  \begin{center}
  \caption{The list of symbols we use in the definition of GRATA.}
  \begin{tabular}{lr}
    \toprule
    Symbol & Meaning \\ 
    \midrule
    $X$  & Ground Truth Labels \\
    $Y$  & Observations \\
    $D \subset X \times Y$  & Universe of Observable Data \\
    $\{T_i\}_{i \in \mathbb{N}}$  & Sequence of Training Sets \\
    $\mathcal{T} = 2^D$ & All Possible Training Sets \\
    $O: \mathcal{T} \to \mathbb{R}$ & Objective Function \\
    $S: \mathbb{N} \to \mathbb{R}$  & Objective Function History \\
    $\mathcal{S} = \{S: \mathbb{N} \to \mathbb{R}\}$  & All Possible Objective Function Histories \\
    $H: \mathcal{S} \times \mathbb{N} \to \{0,1\}$  & Help Criterion \\
    $\mathcal{C} = \mathcal{T} \times \mathcal{S} \times \mathbb{N}$  & Complete Training Set \\
    $G: \mathcal{C}$  & Help Request \\
    $P: \mathcal{C} \to \{0,1\}$  & Stopping Criterion \\
    $L: \mathcal{C} \to D$  & Sample Proposal \\
  \bottomrule
  \end{tabular}
  \end{center}
\end{table}



Applying GRATA to a problem involves defining the above quantities and implementing the high level algorithm
in the figure ~\citep{}.

\begin{figure}
  \textbf{GRATA}
  \begin{algorithmic}
  \STATE $t\gets 1$
  \STATE $c\gets 1$
  \STATE $T_1 = \emptyset$
  \STATE $S(1)\gets 0$
  \STATE $i\gets 1$
  \WHILE{$c$}
    \IF {$H(S,i) == 1$}
      \STATE $G(T_i, S,i)$
    \ELSE
      \STATE $d\gets L(T_i, S, i)$
      \STATE $T_{i+1}\gets T_i \cup \{d\}$
    \ENDIF
    \STATE $i\gets i+1$
    \STATE $S(i)\gets O(T_i)$
    \STATE $c\gets P(T_i, S, i)$
  \ENDWHILE
  \end{algorithmic}
  \caption{The high-level generalized robotic automatic training algorithm (GRATA) which we employ.}
\end{figure}

\subsection{GRATA Simulation}
Understanding the behavior of GRATA requires running the system many times. This would be impractical
to do on the actual robot. It is necessary use a simulator for the process so that we can dramatically shorten the
experimental iteration time. A dense sampling strategy is a good baseline and the data collected can
be queried by an active learning technique to simulate the process of data collection.

Using a dense sampling strategy, we approximate the set $D$ and can then run offline experiments using other
sampling strategies. However, the numbers we report are on actual executions of the system where
the robot gathers the data online.


\section{Applications of GRATA}

\subsection{Recognition Training}

\subsection{Pose Estimation Training}

\subsection{Grasp Training}
4. Once aiming is complete, save the image in the reticle, try to grasp, and save whether that grasp completed.
This can be ascertained by the gripper position.

5. Calculate a density map for this 

\subsection{PID Parameter Training}


\section{Experimental Setup}
\jgonote{This is where we describe the experiments we performed.}
We are not trying to extend the state-of-the-art on our individual tasks. Rather,
we are providing an interactive framework which will raise the maximum automated vision
available to the average user.

Our system can be evaluated in two important ways. Firstly, how effective is 
the system at the tasks for which it is trained? Secondly, how accessible to users is the system?
For now we evaluate system performance, and leave user studies for future work.

\subsection{Object Detection and Pose Estimation}
For object Detection and pose estimation, we constructed data
sets on which we could evaluate our models. This involved hand
annotating the ground truth for the images in the sets, which is a
costly procedure which we are attempting to eliminate for future tasks.
However, we cannot evaluate our system in a principled fashion without such
a data set.

We demonstrate our method's success in this setting where we pay the cost
to acquire the data so that we can trust our method and that cost need not be 
payed during future applications. 

Probably uses confusion rates as the objective function.

Expert viewpoint collection (uses at least hard negatives)

Super dense sampling

uniform hard negative sampling with stopping criterion

\subsection{Grasp Experiments}
For grasp experiments, we conducted online trials in order to compute success rates.
This uses grasp success rate as an objective function.

Expert annotation of grasps

Uniform grasp sampling

Thompson sampling

\subsection{PID Control Experiments}
For PID experiments, we conducted online trials in order to compute success rates.
We use the time to convergence as the objective function




\section{Evaluation and Discussion}
We could report the performance of the system as a function of user interactions.

We could report the performance of the system as a function of program lifetime.

Our representative set could consist of a block, a spoon,
a bowl, a diaper, and a sippy cup. A \emph{single cut video} showing multiple grasps
of all objects is available here.

\subsection{Object Detection}
We establish a baseline for performance by training the system in a representative domain
specific setting, which tells us how well it can perform on laboratory objects when trained by an 
expert. This represents the best that the system could be expected to perform.

\begin{table}
  \begin{center}
  \caption{Performance of our system on the object detection task.}
  \begin{tabular}{lr}
    \toprule
  Data Collection Method & Success Rate \\ 
  \midrule
  Expert Annotation & 0.0 \\ 
  Dense Sampling & 0.0 \\ 
  Hard Negatives Auto-Stopping & 0.0 \\ 
  \bottomrule
  \end{tabular}
  \end{center}
\end{table}

\subsection{Pose Estimation}

\begin{table}
  \caption{Performance of our system on offline data.}
  \begin{center}
  \begin{tabular}{lr}
\toprule
  Data Set            &  \\ 
\midrule
  Expert Curated      & 0.0 \\ 
  Expert (noisy)      & 0.0 \\ 
  Automatic (curated) & 0.0 \\ 
  Automatic (noisy)   & 0.0\\
\bottomrule
  \end{tabular}

  \end{center}
\end{table}


\subsection{Grasping}

\begin{figure}
  \begin{center}
  \begin{tabular}{lr}
  \toprule
  Grasp Sampling Method & Success Rate \\ 
  \midrule
  Expert Annotation & 0.0 \\ 
  Uniform Sampling & 0.0 \\ 
  Thompson Sampling & 0.0 \\
  \bottomrule
  \end{tabular}
  \caption{Performance of our system on the grasping task.}
  \end{center}
\end{figure}

\subsection{PID Control}

\begin{figure}
  \begin{center}
  \begin{tabular}{lr}
    \toprule
  Parameter Learning Method & Average Convergence Time \\ 
  \midrule
  Expert Annotation & 0.0 \\ 
  Constant Learning Rate & 0.0 \\
  Decaying Learning Rate & 0.0 \\
  Wide Scale Random Noise & 0.0 \\
  \bottomrule
  \end{tabular}
  \caption{Performance of our system on the PID control task.}
  \end{center}
\end{figure}

%\subsection{Laboratory Automatic Training}
%How well does the automatic training system perform when trained in laboratory conditions?

%\subsection{Non-Expert In-The-Wild Training}
%We then go on to compare the performance of the system when trained to various degrees by naive
%and technical non-expert users.
%We repeat the experiments with naive users in two settings. In the first, they train laboratory
%objects. In the second, they provide their own objects.


\section{Related Work}

\citet{velez11} created a mobile robot that explores the environment
and actively plans paths to acquire views of objects such as doors.
However it uses a fixed model of the object being detected rather than
updating its model based on the data it has acquired from the
environment.

Methods for planning in information space \citep{he08, atanasov13,
  prentice09} have been applied to enable mobile robots to plan
trajectories that avoid failures due to inability to accurately
estimate positions.  Our approach is focused instead on
object detection and manipulation, actively acquiring data for use
later in localizing and picking up objects. \stnote{May need to say
  more here depending on what GRATA actually is.}


Early models for pick-and-place rely on has been studied since the
early days of robotics~\citep{brooks83, lozano89}.  These systems
relied on models of object pose and end effector pose being provided to the
algorithm, and simply planned a motion for the arm to grasp.  Modern
approaches use object recognition systems to estimate pose and object
type, then libraries of grasps either annotated or learned from
data~\citep{saxena08, goldfeder09, morales03}.  These approaches
attempt to create systems that can grasp arbitrary objects based on
learned visual features or known 3d configuration.  Collecting these
training sets is an expensive process and is not accessible to the
average user in a non-robotics setting.  If the system does not work
for the user's particular application, there is no easy way for it to
adapt or relearn.  Our approach, instead, enables the robot to
autonomously acquire more information to increase robustness at
detecting and manipulating the specific object that is important to
the user at the current moment.

Visual-servoing based methods~\citep{chaumette06} \stnote{Need a whole
  paragraph about that. }

\stnote{\citet{ciocarlie14} seems highly relevant, could not read from
  the train's wifi.}

\citet{collect14} describes an approach for lifelong robotic object
discovery, which infers object candidates from the robot's perceptual
data.  This system does not learn grasping models and does not
actively acquire more data to recognize, localize, and grasp the
object with high reliabilitiy.  It could be used as a first-pass to
our system, after which the robot uses an active method to acquire
additional data enablign it to grasp the object.

Our approach is similar to the philosphy adopted by Rethink Robotic's
Baxter robot, and indeed, we use Baxter as our test
platform~\citep{fitzgerald13}.  \stnote{Haven't actually read this
  paper, just making stuff up based on Rod's talks.  Should read the
  paper and confirm.}  Baxter's manufacturing platform is designed to
be easily learned and trained by workers on the factory floor.  The
difference between this system and our approach is we rely on the
robot to autonomously collect the training information it needs to
grasp the object, rather than requiring this training information to
be provided by the user.


Robot systems for cooking~\citep{bollini12, beetz11} or furniture
assembly~\citep{knepper13} use many simplifying assumptions, including
pre-trained object locations or using VICON to solve the perceptual
system.  We envision vision or RGB-D based sensors mounted on the
robot, so that a person can train a robot to recognize and manipulate
objects wherever the robot finds itself.


\section{Conclusion}

\stnote{First paragraph:  contributions.  What are the things this paper has done to advance the state of the art?}

\stnote{Next paragraphs: future work, spiraling upward to more and
  more ambitiuos extensions.}

Right now, NODE runs on Baxter. We will port NODE to PR2 and other AH systems.
GRATA could be applied in other domains as well.  What are some examples?

% 1. Experimental
%  a. Contribution (is it new?)
%  b. Execution (does it work?)
%  c. Demonstration (is it well understood?)
% 2. Bureaucratic
%  a. Do we know what we are talking about?
%  b. Are we polite about it? (or are we accidently claiming false novelty?)
%  c. Can they understand it? (no algebraic geometry plz k thnx)
% 3. Aesthetic
%  a. The dual of 2c.
%  b. Are spelling, grammar, and figure placement acceptable?
%  c. Does it scan? (how is the meter?)

\bibliographystyle{plainnat}
\bibliography{main,references}


\end{document}








