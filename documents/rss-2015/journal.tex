
We approximate the gradiant using differences after smoothing the
training image:
\begin{align}
\frac{\partial I(x, y)}{\partial x} \approx \frac{I(x + 1, y) - I(x - 1, y)}{2}\\
\frac{\partial I(x, y)}{\partial y} \approx \frac{I(x, y + 1) - I(x, y - 1)}{2}
\end{align}





We validate this
component during training using the 3d IR map of the object, which we
back-project into the original image to obtain the true bounding box.






\subsubsection{Detecting Objects Using BING and Integral Images}

To detect candidate objects, we first apply the BING objectness
detector~\citep{cheng14} to the image, which returns a set $\{B_i\}$
of thousands of approximate object bounding boxes in the image, shown
in Figure~\ref{fig:bing}. This process substantially reduces the
number of bounding boxes we need to consider but is still too large to
process in real time. Besides, even good bounding boxes from BING are
typically not aligned to the degree that we require. Therefore, we use
integral images to efficiently compute the per-pixel map:
\begin{align}
J(p) = \sum_{B \in \{B_i\} s.t. p \in B} \frac{1}{Area(B)}.
\end{align}

This map appears in Figure~\ref{fig:objectness}.  We then apply the
Canny edge detector with hysteresis ~\citep{canny86} to find the
connected components of bright regions in the map $J(p)$, which
correspond with high probability to objects in the image. We form our
candidate object bounding boxes by taking the smallest bounding box
which surrounds each connected component, shown in
Figure~\ref{fig:bounding_boxes}.  These bounding boxes make it easy to
gather training data and to perform inference in real time, but at the
expense of poorly handing occulusion as overlapping objects are fused
into the same bounding box.  It is possible to search within the
proposed bounding boxes to better handle occlusion.
Figure~\ref{fig:object_detection} shows images from each step in this
pipeline, ending with just one candidate bounding boxes for an objects
on an empty table.  Note that we designed this pipeline to quickly and
accurately provide bounding boxes in the presence of relatively
unobstructed backgrounds to support the training process;
Section~\ref{sec:mapping} describes our approach to recognition under
clutter and occlusion.

