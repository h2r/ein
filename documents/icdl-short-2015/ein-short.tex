
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference,onecolumn]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}

\usepackage[numbers]{natbib}
\usepackage{xspace}
\usepackage{graphicx}

\usepackage{subfigure}
\newcommand{\algorithmCTxt}{Ordered Confidence Bound\xspace}
\newcommand{\algorithmCFname}{OrderedConfidenceBound}
\newcommand{\algorithmDTxt}{Prior Confidence Bound\xspace}
\newcommand{\algorithmDFname}{PriorConfidenceBound}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Learning to Pick Up Objects Through Active Exploration}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{John Oberlin and Stefanie Tellex}
\IEEEauthorblockA{Computer Science Department, Brown University}}


% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}

Robotics will assist us at childcare, help us cook, and provide
service to doctors, nurses, and patients in hospitals. Many of these
tasks require a robot to robustly perceive and manipulate objects in
its environment, yet robust object manipulation remains a challenging
problem.  Transparent or reflective surfaces that are not visible in
IR or RGB make it difficult to infer grasp points~\citep{lysenkov13}.
A common source of error is the presence of latent dynamics that
emerge from interactions between the object and the robot's gripper.
For example, a heavy object might fall out of the robot's gripper
unless it grabs it close to the center.

To address these limitations, we propose an approach for enabling a
robot to learn about an object through exploration and adapt its
grasping model accordingly.  We frame the problem of model adaptation
as identifying the best arm for an N-armed bandit
problem~\citep{thompson33} where the robot aims to minimize simple
regret after a finite exploration period~\citep{bubeck09}.  Our robot
can obtain a high-quality reward signal (although sometimes at a
higher cost in time and sensing) by actively collecting additional
information from the environment, and use this reward signal to
adaptively identify grasp points that are likely to succeed.  

Existing algorithms for best arm identification require pulling all
the arms as an initialization step~\citep{mannor04, audibert10,
  chen14}; in the case of identifying grasp points, where each grasp
takes more than 90 seconds and there are more than 1000 potential
arms, this is a prohibitive expense.  To address this problem, we
present a new algorithm, \algorithmDTxt, based on Hoeffding
races~\citep{maron93}. In our approach, the robot pulls arms in an
order determined by a prior, which allows it to try the most promising
arms first. It can then autonomously decide when to stop by bounding
the confidence in the result.  Figure~\ref{fig:ruler} shows the
robot's performance before and after training on a ruler; after
training it grasps the object in the center, improving the success
rate.
\begin{figure}
\subfigure[Before learning.]{\includegraphics[width=0.25\linewidth]{figures/dropping_ruler.png}}%
\subfigure[After learning.]{\includegraphics[width=0.25\linewidth]{figures/holding_ruler.png}}
\subfigure[Objects in our evaluation, which improves grasping performance from 55\% (before learning) to 75\% (after learning).\label{fig:object_glory_shot}]{\includegraphics[width=0.5\linewidth]{figures/object_glory_shot.jpg}}
\caption{Before learning, the robot grasps the ruler near the end, and
  it twists out of the gripper and falls onto the table;
  after learning, the robot grasps near the ruler's center of
  mass.\label{fig:ruler}}
\end{figure}

%% We use slower, more accurate sensing approaches
%% to provide supervision for faster, simpler methods that excel with
%% large amounts of training data.  For example, to perform grasping we
%% use an analytic model to select grasp points, but depending on the
%% object, the best grasp according to the analytic model may not be
%% optimal; the robot can learn better grasps for that object using the
%% analytic model as a prior and actively collecting data for that
%% object.

%%  a view-based model for closed-loop visual picking.  View-based
%% methods for instance detection have many advantages over methods using
%% 3D models, because they directly capture the visual appearance of the
%% object, and are relatively simple and efficient to implement because
%% they operate on low-level features~\citep{hsiao13}.  However these
%% systems require large amounts of training data for robust performance,
%% for example more than 2000 images which must be manually collected and
%% annotated with bounding boxes for the state of the art LINE2D
%% method~\citep{hinterstoisser12}.  Moreover, for manipulation,
%% view-based methods do not propose potential grasps, and autonomous
%% methods for recognizing visual grasps are prone to error. 

%% To address these issues, we present an approach for enabling a robot
%% to train its own view-based model to recognize and manipulate the
%% specific objects it will need to use during collaborations with
%% humans.  Using our algorithm, the robot detects candidate objects for
%% training using a depth sensor, then actively collects view-based
%% visual templates to perform robust instance-based object detection,
%% pose estimation and closed-loop grasping using visual servoing.
%% Because our camera can move with seven degrees of freedom, the robot
%% can collect large quantities of data leading to simple visual models
%% that perform with high accuracy even under occlusion.  Our approach is
%% enabled by three innovations: our end-to-end algorithm for collecting
%% view-based training data with supervision obtained from a
%% higher-reliability depth sensor, which is supported by a simple and
%% robust method for determining candidate grasps using a depth sensor
%% mounted on a seven-degree-of-freedom arm, along with an approach for
%% autonomously and reliably finding object bounding boxes once the
%% object is on a background such as a floor or table.

We evaluated \algorithmDTxt on a Baxter robot, demonstrating that our
adaptation step improves the overall pick success rate from 55\% to
75\% on our test set of 30 household objects, shown in
Figure~\ref{fig:object_glory_shot}.  Moreover, our approach also
enables the robot to learn success probabilities for each object it
encounters; when the robot fails to infer a successful grasp for an
object, it knows this fact, enabling it to take active steps to
recover such as asking for help~\citep{tellex14}.

We first give an overview of our object detection and localization pipeline.
Next we formalize our grasping framework as a bandit problem, where each
arm corresponds to a grasp point on the object.
Section~\ref{sec:evaluation} describes our evaluation in simulation
and on the real robot with $30$ household objects,
Section~\ref{sec:relatedwork} covers related work, and
Section~\ref{sec:conclusion} concludes.


\section{Conclusion}

We presented a formalization of the grasping problem as best arm
identification in an N-armed bandit.  Our bandit problem has 1764
levers, so even if we could pull a lever every 10 seconds it would
take around 5 hours to explore all of the arms for a single object.
To address this problem, we created a new algorithm, \algorithmDTxt,
which explores promising arm first by exploiting prior knowledge,
significantly reducing constant factors. 

Our stack gathers feedback from the environment, which it uses to
learn models to detect, localize, and manipulate previously unseen
objects. In the future, we plan to explore learning parameters for a
wider variety of tasks.  For instance, different objects can be
located and oriented better or worse at different heights. One could
learn which heights work well for which objects.  Likewise, we use
color gradients for localization, but some objects would work better
with other quantities. One could learn the appropriate map to use when
localizing each object, or even further, the map might also depend
upon the robot's current environment.

%Right now, NODE runs on Baxter. We will port NODE to PR2 and other AH systems.
%GRATA could be applied in other domains as well.  What are some examples?

Our stack currently runs on Baxter, but the requirements are not
stringent.  In fact, it would be possible to execute all scanning and
some training for crane grasps on a modified 3D printer. Furthermore,
the parallel electric gripper for Baxter is more difficult to infer
grasps for than the gripper on the PR2.  Even though the PR2 lacks the
IR rangefinder we use, that data could be gathered by a printer
converted from a scanner, and the PR2 could perform its own grasp
training.

It is clear that the system's accuracy and precision would benefit
from the use of more sophisticated imaging equipment such as the
Kinect 2. Better and faster point clouds acquisition would allow the
use of more precise physical models for grasps. It would also open the
way for additional grasp types, such as side and handle grasps.

Instance-based approaches mean that the robot must acquire data about
a specific object before manipulating it; however by adapting itself
to the object it can obtain higher accuracy.  We aim to aggregate data
collected via our instance-based system to create a new data set of
images, RGB-D, and grasp success rates at various poses to provide
supervision for general-purpose category models for grasping.  Our
long term vision is a system that can infer high quality grasps for
any object, but adaptively recover if the initial grasp attempt is
unsuccessful, leading overall to robust and accurate pick-and-place.



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
{\small
\bibliographystyle{plainnat}
\bibliography{main}
}



% that's all folks
\end{document}


