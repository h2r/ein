%%%% ijcai15.tex

% These are the instructions for authors for IJCAI-15.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.

\documentclass{article}
% The file ijcai15.sty is the style file for IJCAI-15 (same as ijcai07.sty).
\usepackage{ijcai15}
\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{color}
%\usepackage{algorithmic}
\usepackage[]{algorithm2e}
\usepackage{hyperref}
\hypersetup{letterpaper,bookmarksopen,bookmarksnumbered,
pdfpagemode=UseOutlines,
colorlinks=true,
linkcolor=blue,
anchorcolor=blue,
citecolor=blue,
filecolor=blue,
menucolor=blue,
urlcolor=blue
}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{latexsym,amssymb,amsmath} % for \Box, \mathbb, split, etc.
\newcommand{\stnote}[1]{\textcolor{blue}{\textbf{ST: #1}}}
\newcommand{\jgonote}[1]{\textcolor{green}{\textbf{JGO: #1}}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{argmax}}\;}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}\;}


% Use the postscript times font!
\usepackage{times}
\newcommand{\algorithmCTxt}{Ordered Confidence Bound\xspace}
\newcommand{\algorithmCFname}{OrderedConfidenceBound}
\newcommand{\algorithmDTxt}{Prior Confidence Bound\xspace}
\newcommand{\algorithmDFname}{PriorConfidenceBound}

% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Bandit-Based System Adaptation for Grasping}


\author{}

\begin{document}

\maketitle

\begin{abstract}
A key aim of current research is to create robots that can reliably
manipulate objects.  However, in many instances, information needed to
infer a successful grasp is latent in the environment and arises from
complicated physical dynamics; for example, a heavy object might need
to be grasped in the middle or else it will twist out of the robot's
gripper.  The contribution of this paper is a formalization of object
picking as an N-armed bandit problem, where each potential grasp point
corresponds to an arm with unknown pick success rate.  This
formalization enables us to apply bandit-based exploration algorithms
to enable a robot to identify the best arm by attempting to pick up
the object and tracking its successes and failures.  Because the
number of grasp points is very large, we define a new algorithm for
best arm identification in budgeted bandits that computes confidence
bounds while incorporating prior information, enabling the robot to
quickly find a near optimal arm without pulling all the arms as in
UCB-based approaches.  We demonstrate that our adaptation step
significantly improves accuracy over a non-adaptive system, enabling a
robot to improve grasping models through experience.


%%   To address this
%% problem, we focus not on {\em category-based} manipulation (pick up
%% any mug) but rather {\em instance-based} manipulation (pick up this
%% mug).  

%% Our framework runs on an unmodified Baxter
%% robot; using our algorithm, a robot can interact with an object for
%% twenty minutes, and then reliably and quickly localize it with vision
%% and pick it up with closed-loop visual servoing. 

%% Instance-based recognition and pose estimation can be highly
%% accurate but require the designer to adapt the system to the specific
%% object being manipulated in large and small ways, ranging from
%% collecting training data, to choosing parameter values, to selecting
%% algorithms for tasks such as image segmentation or bounding box
%% classification.  

\end{abstract}



\section{Introduction}


Robotics will assist us at childcare, help us cook, and provide
service to doctors, nurses, and patients in hospitals. Many of these
tasks require a robot to robustly perceive and manipulate objects in
its environment, yet robust object manipulation remains a challenging
problem.  Systems for general-purpose manipulation are computationally
expensive and do not enjoy high accuracy on novel
objects~\citep{saxena08}.  A common source of error is the presence of
latent dynamics that emerge from interactions between the object and
the robot's gripper.  For example, a heavy object might fall out of
the robot's gripper unless it grabs it close to the center.
Transparent or reflective surfaces that are not visible in IR or RGB
make it difficult to infer grasp points~\citep{lysenkov13}.

To address these limitations, we propose an approach for enabling a
robot to learn about an object through exploration and adapt its
grasping model accordingly.  We frame the problem of model adaptation
as identifying the best arm for an N-armed bandit
problem~\citep{thompson33} where the robot aims to minimize simple
regret after a finite exploration period~\citep{bubeck09}.  Our robot
can obtain a high-quality reward signal (although sometimes at a
higher cost in time and sensing) by actively collecting additional
information from the environment, and use this reward signal to
adaptively identify grasp points that are likely to succeed.  

Identifying the best grasp point corresponds to best arm
identification with pure exploration in bandit problems.  Existing
algorithms for best arm identification require pulling all the arms as
an initialization step~\citep{mannor04, audibert10, chen14}, a
prohibitive expense when each arm pull takes on the order of 90
seconds and there are more than 1000 arms.  To address this problem,
we present a new algorithm, \algorithmDTxt, based on Hoeffding
races~\citep{maron93}. In our approach, the robot pulls arms in an order
determined by a prior, which allows it to try the most promising arms
first. It can then autonomously decide when to stop by bounding the
confidence in the result.  Figure~\ref{fig:ruler} shows the robot's
performance before and after training on a ruler; after training it
grasps the object in the center, improving the success rate.
\begin{figure}
\subfigure[Before learning.]{\includegraphics[width=0.5\linewidth]{figures/dropping_ruler.png}}%
\subfigure[After learning.]{\includegraphics[width=0.5\linewidth]{figures/holding_ruler.png}}
\caption{Before learning, the robot grasps the ruler near the end, and it twists out of its gripper and falls onto the table when it lifts; after learning, the robot knows to grasp it near the center of mass.\label{fig:ruler}}
\end{figure}

%% We use slower, more accurate sensing approaches
%% to provide supervision for faster, simpler methods that excel with
%% large amounts of training data.  For example, to perform grasping we
%% use an analytic model to select grasp points, but depending on the
%% object, the best grasp according to the analytic model may not be
%% optimal; the robot can learn better grasps for that object using the
%% analytic model as a prior and actively collecting data for that
%% object.

%%  a view-based model for closed-loop visual picking.  View-based
%% methods for instance detection have many advantages over methods using
%% 3D models, because they directly capture the visual appearance of the
%% object, and are relatively simple and efficient to implement because
%% they operate on low-level features~\citep{hsiao13}.  However these
%% systems require large amounts of training data for robust performance,
%% for example more than 2000 images which must be manually collected and
%% annotated with bounding boxes for the state of the art LINE2D
%% method~\citep{hinterstoisser12}.  Moreover, for manipulation,
%% view-based methods do not propose potential grasps, and autonomous
%% methods for recognizing visual grasps are prone to error. 

%% To address these issues, we present an approach for enabling a robot
%% to train its own view-based model to recognize and manipulate the
%% specific objects it will need to use during collaborations with
%% humans.  Using our algorithm, the robot detects candidate objects for
%% training using a depth sensor, then actively collects view-based
%% visual templates to perform robust instance-based object detection,
%% pose estimation and closed-loop grasping using visual servoing.
%% Because our camera can move with seven degrees of freedom, the robot
%% can collect large quantities of data leading to simple visual models
%% that perform with high accuracy even under occlusion.  Our approach is
%% enabled by three innovations: our end-to-end algorithm for collecting
%% view-based training data with supervision obtained from a
%% higher-reliability depth sensor, which is supported by a simple and
%% robust method for determining candidate grasps using a depth sensor
%% mounted on a seven-degree-of-freedom arm, along with an approach for
%% autonomously and reliably finding object bounding boxes once the
%% object is on a background such as a floor or table.

We evaluated \algorithmDTxt on a Baxter robot, demonstrating that our
adaptation step improves the overall pick success rate from 55\% to
75\% on our test set of 30 household objects, shown in
Figure~\ref{fig:object_glory_shot}.  Moreover, our approach also
enables the robot to learn success probabilities for each object it
encounters; when the robot fails to infer a successful grasp for an
object, it knows this fact, enabling it to take active steps to
recover such as asking for help~\citep{tellex14}.

We first give an overview of our object detection and localization pipeline.
Next we formalize our grasping framework as a bandit problem, where each
arm corresponds to a grasp point on the object.
Section~\ref{sec:evaluation} describes our evaluation in simulation
and on the real robot with $30$ household objects,
Section~\ref{sec:relatedwork} covers related work, and
Section~\ref{sec:conclusion} concludes.


%% Our software is compatible with ROS and the Baxter SDK version 1.0.0,
%% and we intend to release it as free software should our paper be
%% accepted.  As more and more instance-based models are collected, this
%% corpus will form a unique training set for category-based models for
%% detecting and grasping novel objects, since the robot will have a very
%% large number of views of a large set of objects as well as storing
%% depth information and grasping success.

\section{Object Detection and Localization}
Our object detection and pose estimation pipeline uses conventional
computer vision algorithms in a simple software architecture to
achieve a frame rate of about 2Hz for object detection and pose
estimation.  Object classes consist of object instances rather than
pure object categories.  Using instance recognition means we cannot
reliably detect categories, such as ``mugs,'' but the system will be
much better able to detect, localize, and grasp the specific instances for which
it does have models.

Our detection pipeline runs on stock Baxter with one additional
computer, using the arm cameras and IR sensor to localize objects in
the environment.  Our recognition pipeline takes video from the
robot's wrist cameras, proposes a small number of candidate object
bounding boxes in each frame, and classifies each candidate bounding
box as belonging to a previously encountered object class.

When the robot moves to attempt a pick, it uses detected bounding
boxes and visual servoing to move the arm to a position approximately
above the target object. Next it uses image gradients to servo the arm
to a known position and orientation above the object. Because we can
know the arm's position relative to the object, we can reliably
collect statistics about the success rate of grasps at specific points
on the object.


\begin{figure}
  \centering
\includegraphics[height=1in]{figures/ruler_rgb.png}
\includegraphics[height=1in]{figures/ruler_ir.png}
\caption{Automatically acquired RGB image for one of the objects in our
  training set, combined with the IR raster scan.\label{fig:ir}}
\end{figure}

To infer grasp points, we create a depth map of the object using the
robot's IR sensor.  (This process would be much faster with an RGB-D
sensor such as the Kinect, but the advantage of our approach is that
it can be used with a stock Baxter with no additional sensing.)  We
collect many measurements for the pointcloud by performing an overhead
raster scan of the object. The IR sensor reports at about 30Hz but
gives fairly good localization.  In order to use the good resolution
despite the sparsity of measurement induced by the frequency of the
sensor and the speed of the robot's movement, we employ a Parzen
kernel density estimator to record z measurements at 1mm $(x,y)$
resolution. We then downsample to a $21 \times 21$ grid at 1cm
resolution, which yields a clean depth map and a tractable state space
for grasp inference. We consider 4 gripper orientations at each
$(x,y)$ location in the scan for a total of 1764 possible grasps. Our
prior for grasp success is a map $\pi:(x,y,\theta) \rightarrow \mathbb{R}$
generated by convolving the 1cm depth map with 4 linear filters which
correspond to the 4 orientations we consider. The responses are
scaled linearly to be between 0 and 1 inclusive.

This pipeline works well on some objects, but often fails for a
variety of reasons.  For example, our grasp model is not able to 
reliably infer grasps for objects that are not visible in IR, such as
transparent or very dark objects.  Other objects need to be
grasped in particular locations to avoid getting stuck on the gripper
due to the compliance of the object and the dynamics of the robot's
gripper.  The next section defines how we adapt this grasping pipeline
by learning to identify good grasps from experience.

\section{Bandit-based Adaptation}

The formalization we contribute treats grasp learning as an N-armed bandit problem. Because
the problem domain is rooted in the physical world, we can obtain
high-quality supervision at the cost of time and additional sensing by
trying grasps with the robot. Formally, the agent is given an N-armed
bandit, where each arm pays out $1$ with probability $\mu_i$ and $0$
otherwise.  The agent's goal is to identify a good arm (with payout
$>= k$) with probability $c$ (e.g., 95\% confidence that this arm is
good) as quickly as possible.  As soon as it has done this, it should
terminate.  The agent is also given a prior $\pi$ on the
arms. \algorithmCTxt only makes use of order of the arms as ranked by
the prior, trying promising ones first and ignoring the specific
values of $\pi$. \algorithmDTxt, on the other hand, actually makes use
of the probabilities specified by the prior in order to explore more
aggressively.


Our first algorithm, \algorithmCTxt, iterates through each arm, and tries it
until it has identified that it either is above $k$ with probability
$c$ (in which case it terminates) or it is below $k$ with probability
$c$ (in which case it moves to the next arm in the sequence.
Pseudo-code appears in Algorithm~\ref{alg:bandit1}.  To compute this
probability we need to estimate the probability that the true payout
probability, $\mu$ is greater than the threshold, $c$ given the
observed number of successes and failures:
\begin{align}
\Pr(\mu_i > c|  S, F)
\end{align}

We can compute this probability using the law of total probability:
\begin{align}
\Pr(\mu_i > c|  S, F) &= \int_k^1 \Pr(\mu_i=\mu | S, F) d\mu\\
\intertext{We assume a beta distribution on $\mu$:}
                      &= \int_k^1 \mu^S (1- \mu) ^F d\mu
\end{align}

This integral is the CDF of the beta distribution, and is called the
regularized incomplete beta function~\citep{olver10}.  Note that if
$\mu = k$ the run time is unbounded, so we fix an $\epsilon > 0$ and
accept a grasp if $\mu \in [k-\epsilon, k+\epsilon]$ with probability
$c$.


\begin{algorithm}
\SetKwFunction{x}{\algorithmDFname}
\x{$\pi$, $k$, $\delta_{accept}$, $\delta_{reject}$, $maxTries$}

\For {$i \in 0...n$}{
    $S_i  \gets \pi(i)$
}
$\mbox{Initialize } F_0 \dots F_n \mbox{ to } 0$\\
$\mbox{Initialize } M_0 \dots M_n \mbox{ to } 0$\\
$totalTries \gets 0$\\
\While {$true$}{
  $totalTries \gets totalTries + 1$\\
  \For {$i \in 0...n$}{
      $M_i  \gets S_i / (S_i + F_i)$\\
  }

  %$\tilde{M}_i \gets sortDescending(M_i)$ \\

  $maxI \gets -1$\\
  $maxIval \gets -1$\\
  \For {$i \in 0...n$}{
    $p^i_{below} \gets \int_0^k \Pr(\mu_i = \mu | S_i, F_i) d\mu $\\
    \If{$p_{below} < \delta_{reject}$ and $M_i > maxIval$}{
      $maxI \gets i$\\
      $maxIval \gets M_i$\\
    }
  }

  $j \gets maxI$\\

  $r \gets sample(arm_j)$\\
  \uIf {$r = 1$}{
    $S_j  \gets S_j + 1$
  }\Else{
    $F_j  \gets F_j + 1$
  }
  $p_{below} \gets \int_0^k \Pr(\mu_j = \mu | S_j, F_j) d\mu $\\
  $p_{above} \gets \int_k^1 \Pr(\mu_j = \mu | S_j, F_j) d\mu $\\ %1 - p_{below}$\\
  $p_{threshold} \gets \int_{k - \epsilon}^{k + \epsilon} \Pr(\mu_j = \mu | S_j, F_j) d\mu $\\

  \uIf{$p_{above} \ge  \delta_{accept}$}{
    return $j$ \tcp*[r]{accept this arm}
  }\uElseIf{$p_{threshold} \ge \delta_{accept}$}{
    return $j$ \tcp*[r]{accept this arm}
  }\uElseIf{$totalTries > maxTries$}{
    \For {$i \in 0...n$}{
	$M_i  \gets S_i / (S_i + F_i)$\\
    }
    $maxI \gets -1$\\
    $maxIval \gets -1$\\
    \For {$i \in 0...n$}{
      \If{$M_i > maxIval$}{
	$maxI \gets i$\\
	$maxIval \gets M_i$\\
      }
    }
    return $maxI$ \tcp*[r]{return the best arm encountered}
  } \Else{
    pass \tcp*[r]{keep trying}
  }
}

\caption{\algorithmDTxt for Best Arm
  Identification\label{alg:bandit2}}
\end{algorithm}


%% * terminate fast, and know when to terminate
%% * identify the best grasp with high probability (and search longer if you haven't)
%% * providing a prior
%% * providing a bound on mu. 

%To obtain the 3D point cloud maps, we use the IR sensor in
%conjunction with the RGB camera.

%% The IR sensor is a triangulation rangefinder, which means it actively
%% illuminates its target with an infrared LED. If the robot is totally
%% deprived of light, the spot of light projected by the IR LED can be
%% seen in the RGB camera image. The location of the point in the image
%% changes significantly with the range of the point being measured. We
%% record the location at several reference ranges and interpolate
%% between the two closest reference points, allowing us to know the
%% pixel in the RGB image corresponding to the $(x,y,z)$ point being
%% measured. Together, this forms an $(r,g,b,x,y,z)$ measurement for the
%% pointcloud.

\section{Evaluation}
\label{sec:evaluation}

\begin{figure}
\includegraphics[width=1\linewidth]{figures/object_glory_shot.jpg}
\caption{The objects used in our evaluation.\label{fig:object_glory_shot}}
\end{figure}


The aim of our evaluation is to assess the ability of the system to
acquire visual models of objects which are effective for grasping and
object detection.  We first assess our approach in simulation,
comparing both algorithms to Thompson sampling as a baseline.  Next we
describe our robotic evaluation, which assesses our
system's ability to learn and adaptively improve its ability to grasp
objects, end-to-end.

\subsection{Simulation}
Our simulated results compare our two algorithms to Thompson sampling, 
assessing the trade-offs inherent in our choice
of parameters and algorithms.  We present results in

\jgonote{This should be updated for whatever we decide to do. Here and
previously we need to make sure to refer to algorithms because now we
have two.}

Table~\ref{fig:simulation_results}.  We simulate picking performance
by creating a sequence of $20$ bandits, where each arm pays out at a
rate of $0.1$ except for one, which pays out at $0.9$.  We move the
location of the best arm to a uniformly sampled random position in the
sequence.  Thompson Sampling always uses all trials in its budget and,
given enough trials, reliably finds the optimal arm.  We present two
versions of \algorithmCTxt with different parameters.  The tight
bound uses a confidence interval defined by the union bound to decide
when to move on; this results in the agent pulling each arm many times
before achieving a high confidence.  It almost always terminates with
the optimal arm, but takes many trials to do it.  In contrast, the
parameter settings used in this paper with a lower confidence bound
takes many fewer trials on average, but sometimes terminates with a
non-optimal arm.  Because of the high cost of running on the robot, we
use less conservative settings in our evaluation.



\begin{figure}
\includegraphics{figures/bestarm.pdf}
\caption{Results comparing our approach to various baselines in simulation.\label{fig:simulation_results}}
\end{figure}


\subsection{Robotic Evaluation}

\jgonote{In this section we use algorithm singular because we only evaluated one of them.}

We have implemented our approach on the Baxter
robot. It is equipped with a seven-degree-of-freedom arm with a
camera and IR depth sensor, which we use as a one-pixel depth camera
to acquire our models.

The robot acquired visual and RGB-D models for 30 objects using our
autonomous learning system.  We manually verified that the scans were
accurate, and set the following parameters: height above the object
for the IR scan (to approximately 2cm); this height could be acquired
automatically by doing a first coarse IR scan following by a second IR
scan 2cm above the tallest height, but we set it manually to save
time.  Additionally we set the height of the arm for the initial servo
to acquire the object.

After acquiring visual and IR models for the object at different poses
of the arm, the robot performed the bandit-based adaptation step using
Algorithm~\ref{alg:bandit2}.

We report the performance of the robot at picking using the learned
height for servoing, but without grasp learning, then the number of
trials used for grasp learning by our algorithm, and finally the
performance at picking using the learned grasp location.

After the robot detects an initially successful grab, it shakes the
object vigorously to ensure that it would not fall out during
transport. After releasing the object and moving away, the robot
checks to make sure the object is not stuck in its gripper. If the
object falls out during shaking or does not release properly, the
grasp is recorded as a failure. If the object is stuck, the robot
pauses and requests assistance before proceeding.

Most objects have more than one pose in which they can stand upright
on the table. If the robot knocks over an object, the model taken in
the reference pose is no longer meaningful. Thus, during training, we
monitored the object and returned it to the reference pose whenever
the robot knocked it over. In the future, we aim to incorporate
multiple components in the models which will allow the robot to cope
with objects whose pose can change during training.

Our algorithm used an accept threshold of $0.7$, reject confidence of
$0.95$ and epsilon of $0.2$.  These parameters result in a policy that
rejects a grasp after one failed try, and accepts if the first three
picks are successful.  Different observations of success and failure
will cause the algorithm to try the grasp more to determine the true
probability of success.  The policy for exploring an arm appears in
Figure~\ref{fig:policy}.

%% \begin{figure}
%% \includegraphics{figures/policy.pdf}
%% \caption{Policy for an \algorithmCTxt agent given a belief state, defined by an
%%   observed number of successes and failures on that arm for our model
%%   parameters.  Red (-) shows states where the agent rejects the arm
%%   and move to the next one; green (+) shows states where it accepts
%%   the arm and stops learning, and the arrows show movement to the next
%%   belief state based on whether the next trial is successful.\label{fig:policy}}
%% \end{figure}

\begin{table}
\small
\begin{tabular}{cccc}
\toprule
		    & Prior         &  Training     & Marginal\\
\midrule
\multicolumn{4}{c}{$\Delta = 0; training = 3$} \\
\midrule
% too easy
Brush    	    & 10/10         &  3/3         &  10/10\\
Packing Tape        & 9/10          &  3/3         &  9/10 \\
Purple Marker       & 9/10          &  3/3         &  9/10 \\
Red Bowl    	    & 10/10         &  3/3         &  10/10\\
Red Bucket    	    & 5/10          &  3/3         &  5/10 \\
Shoe    	    & 10/10         &  3/3         &  10/10\\
Stamp    	    & 8/10          &  3/3         &  8/10 \\
Whiteout    	    & 10/10         &  3/3         &  10/10\\
Wooden Spoon        & 7/10          &  3/3         &  7/10 \\
% good improvement  
\midrule
\\
\multicolumn{4}{c}{$\Delta >= 2$} \\
\midrule
Big Syringe    	    & 1/10          &  13/50       &  4/10 \\
Blue Salt Shaker    & 6/10          &  5/10        &  8/10 \\
Bottle Top    	    & 0/10          &  5/17        &  7/10 \\
Garlic Press        & 0/10          &  8/50        &  2/10 \\
Gyro Bowl    	    & 0/10          &  5/15        &  3/10 \\
Metal Pitcher       & 6/10          &  7/12        &  10/10\\
Mug    		    & 3/10          &  3/4         &  10/10\\
Round Salt Shaker   & 1/10          &  4/16        &  9/10 \\
Sippy Cup    	    & 0/10          &  6/50        &  4/10 \\
Triangle Block      & 0/10          &  3/13        &  7/10 \\
Vanilla	   	    & 5/10          &  4/5         &  9/10 \\
Wooden Train        & 4/10          &  11/24       &  8/10 \\

\midrule
\\
\multicolumn{4}{c}{$\Delta <= 1$} \\
\midrule
% little to no improvement
Clear Pitcher       & 4/10          &  3/4         &  4/10 \\
Dragon    	    & 8/10          &  5/6         &  7/10 \\
Epipen    	    & 8/10          &  4/5         &  8/10 \\
Helicopter    	    & 2/10          &  8/39        &  3/10 \\
Icosahedron    	    & 7/10          &  7/21        &  8/10 \\
Ruler    	    & 6/10          &  5/12        &  7/10 \\
Syringe    	    & 9/10          &  6/9         &  10/10\\
Toy Egg    	    & 8/10          &  4/5         &  9/10 \\
Yellow Boat    	    & 9/10          &  5/6         &  9/10 \\
\midrule
Total		    & 165/300       &  148/400     & 224/300\\
Rate		    & 0.55          &  0.37        & 0.75\\
\bottomrule
\end{tabular}
\caption{Results from the robotic evaluation of \algorithmDTxt. We tested on 30 objects. We use
$\Delta$ to denote the difference in success rate between the grasp recommended
by the prior (Prior column) and the grasp learned by the system (Marginal
column). The top block contains objects which succeeded on the initial grasp
estimate and therefor did not learn a new grasp. Since the grasp did not change
we report the results from one round of 10 picks as both the prior and marginal
success rate.  The middle block contains objects which spent some time learning
and improved their performance notably. The bottom block contains objects for
which some learning occurred but little or more improvement was seen. A
performance drop after learning was seen on one class:
Dragon.\label{table:robot_results}}
% Curses to the dragon. Curses to Smaug!
% jgo: I double and triple checked the sums.
\end{table}

\subsection{Discussion}
Consider the top block in Figure~\ref{fig:robot_results}, the objects for which the
initially estimated grasp succeeds 3 times in a row. Note that when 10 trials are performed,
it becomes clear that 3 consecutive successes do not assure even near perfect performance.
Even though \algorithmDTxt rejects more quickly than \algorithmCTxt, they behave identically
for these objects and accept faster than Thompson sampling does. 

% XXX TODO
% talk about successes and failures
The first few grasps suggested by the prior for the Triangle Block were infeasible because
the gripper slid over the sloped edges and pinched the block out of its grippers. The robot
tried grasps until it found one that targeted the sides that were parallel to the grippers,
resulting in a flush grasp. 
For the Round Salt Shaker, the robot first attempted to grab the round plastic dome, which 
is infeasible. It tried grasps until it found one on the handle.

Objects such as the Round Salt Shaker and the Bottle Top are on the edge of tractability
for thorough policies such as Thompson sampling and \algorithmCTxt. \algorithmDTxt, on the other
hand, rejects arms quickly so as to make these two objects train in relatively short order
while bringing even more difficult objects such as the Sippy Cup and Big Syringe into the
realm of possibility. It would have taken substantially more time and picks for Thompson
sampling and \algorithmCTxt to reject the long list of bad grasps before finding the good ones.

The Garlic Press is a geometrically simple object but quite heavy compared to the others. The
robot found a few grasps which might have been good for a lighter object, but it frequently
shook the press out of its grippers when confirming grasp quality.
The Big Syringe has some good grasps which are detected well by the prior, but due to its poor
contrast and transparent tip, orientation servoing was imprecise and the robot was unable to
learn well due to poor signal. What improvement did occur was due to finding a grasp which
consistently deformed the bulb into a grippable shape regardless of the perceived orientation
of the syringe. Similar problems were observed with the Clear Pitcher and Icosahedron.

Some of the objects had several grasps of similar, mediocre quality, which
caused the robot to try multiple grasps several times, eventually accepting one of the
mediocre grasps by chance. It is likely that \algorithmDTxt rejected good grasps a little
too quickly, as a result perhaps even taking longer than \algorithmCTxt would have and settling
for a worse solution.

The Red Bucket exhibited interesting behavior. Its gradient profile is rectangular, but at the
chosen height only two opposing sides are visible. Due to symmetry it was able to find a good
grasp despite the degeneracy in the model. Most of its failures were because the object is tall
and the robot did not back up enough before checking if the gripper was empty, which triggered
false negatives for grasp release.
Finally, some objects, such as the Helicopter and Dragon, barely fit in the gripper and would
therefor benefit from additional servo iterations to increase localization precision. 


Objects which failed entirely did so because of reasonable limitations of the
system induced by compromises we made for global compatibility. For instance,
there is only about a 5mm difference between the width of the Dragon and the width of 
the gripper. If we double the number of iterations
during fine grained servoing we can more reliably pick it, but this would either
introduce another parameter in the system (iterations) or excessively slow down
other objects which are more tolerant to error.


\section{Related Work}

\label{sec:relatedwork}


Early models for pick-and-place rely on has been studied since the
early days of robotics~\citep{brooks83, lozano89}.  These systems
relied on models of object pose and end effector pose being provided
to the algorithm, and simply planned a motion for the arm to grasp.
\citet{bohg13} survey data-driven approaches to grasping.  Our
approach can be thought of as a pipeline for automatically building an
experience database consisting of object models and known good grasps,
using analytic approaches to grasping unknown objects to generate a
grasp hypothesis space and using our bandit-based method for trying
grasps and learning instance-based distributions for the grasp
experience database.  In this way our system achieves the best of both
approaches: models for grasping unknown objects can be applied; when
they do fail, the system can attempt to recover by trying grasps and
adapting itself based on that specific object.  

Modern approaches use object recognition systems to estimate pose and
object type, then libraries of grasps either annotated or learned from
data~\citep{saxena08, goldfeder09, morales03,ciocarlie14}.  These
approaches attempt to create systems that can grasp arbitrary objects
based on learned visual features or the object's known 3d
configuration.  Collecting these training sets is an expensive process
and is not accessible to the average user in a non-robotics
setting. If the system does not work for the user's particular
application, there is no easy way for it to adapt or relearn.  Our
approach, instead, enables the robot to autonomously acquire more
information to increase robustness at detecting and manipulating the
specific object that is important to the user at the current moment.
Other approaches focus on object discovery and
manipulation~\citep{lyubova13, kraft10, collet14, schiebener12}.  By
formalizing grasp identification as a bandit problem, we are able to
leverege existing strategies for inferring the best arm.


Many existing approaches use a simulator to assess planned grasp
quality, because assessing grasp quality in simulation is much faster
than trying it on the real robot~\citet{miller04}.  Our approach,
instead, is more expensive to evaluate grasp quality because it
requires actually attempting to pick up the object.  However, by
assessing grasp quality in the end-to-end system, our approach
potentially obtains more accurate information about grasp quality
during training, that it can leverage later at inference time.


Crowd-sourced and web robotics have created large databases of objects
and grasps using human supervision on the web~\citep{kent14a, kent14}.
These approaches outperform automatically inferred grasps but still
require humans in the loop.  Our approach can incorporate human
annotations in the form of the prior.  If the annotated grasps work
well, then the robot will quickly converge and stop sampling; if they
are poor grasps, our approach will find better ones.


\citet{nguyen14} learn to manipulate objects such as a light switch or
drawer with a similar self-training approach.  Our work learns visual
models for objects for autonomous pick-and-place rather than to
manipulate objects and uses a bandit-based formulation to actively
select the best grasp point.


Methods for planning in information space \citep{he08, atanasov13,
  prentice09} have been applied to enable mobile robots to plan
trajectories that avoid failures due to inability to accurately
estimate positions.  \citet{velez11} created a mobile robot that
explores the environment and actively plans paths to acquire views of
objects such as doors.  However it uses a fixed model of the object
being detected rather than updating its model based on the data it has
acquired from the environment.  Our approach is focused instead on
identifyng the best grasp point by actively experimenting with objects
in the world.

Approaches to plan grasps under pose uncertainty~\citep{stulp11} or
collect information from tacticle sensors~\citep{hsiao10} using
POMDPs.  \citet{platt11} describe new algorithms for solving POMDPs by
tracking belief state with a high-fidelity particle filter, but using
a lower-fidelity representation of belief for planning, and tracking
the KL divergence.  \citet{hudson12} used active perception to create
a grasping system capable of carrying out a variety of complex tasks.
Using feedback is critical for good performance, but the model cannot
adapt itself to new objects.  Our approach could be used to improve
any of these systems by defining a space of parameters to optimize,
such as potential grasp points, and then assessing the performance
from experience.

We formalize the problem as an N-armed bandit~\citep{thompson33} where
the robot aims to perform best arm identification~\citep{audibert10,
  chen14}, or alternatively, to minimize simple regret after a finite
exploration period~\citep{bubeck09}.  \citet{audibert10} explored best
arm identification in a fixed budget setting; however a fixed budget
approach does not match our problem, because we would like the robot
to stop sampling as soon as it has improved performance above a
threshold.  We take a fixed confidence approach as in \citet{chen14},
but their fixed confidence algorithm begins by pulling each arm once,
a prohibativly expensive operation on our robot.  Instead our
algorithm estimates confidence that one arm is better than another
follows Hoeffding races~\citep{maron93} but operates in a confidence
threshold setting that incorporates prior information.  By
incorporating prior information, our approach achieves good
performance without being required to pull all the arms.


\section{Conclusion}

\label{sec:conclusion}

%\stnote{First paragraph:  contributions.  What are the things this paper has done to advance the state of the art?}
%\stnote{Next paragraphs: future work, spiraling upward to more and
  %more ambitiuos extensions.}
%Software stack for autonomously acquiring instance-based models of objects for the Baxter Research Robot.
%Ordered Confidence Bound algorithm for solving pure exploration bandits with low constant factors by exploiting prior knowledge.

We presented a formalization of the grasping problem as best arm
identification in an N-armed bandit.  Our bandit problem has 1764
levers, so even if we could pull a lever every 10 seconds it would
take around 5 hours to explore all of the arms for a single object.
To address this problem, we created a new algorithm, \algorithmDTxt,
which explores promising arm first by exploiting prior knowledge,
significantly reducing constant factors. 

Our stack gathers feedback from the environment, which it uses to
learn models to detect, localize, and manipulate previously unseen
objects. In the future, we plan to explore learning parameters for a
wider variety of tasks.  For instance, different objects can be
located and oriented better or worse at different heights. One could
learn which heights work well for which objects.  Likewise, we use
color gradients for localization, but some objects would work better
with other quantities. One could learn the appropriate map to use when
localizing each object, or even further, the map might also depend
upon the robot's current environment.

%Right now, NODE runs on Baxter. We will port NODE to PR2 and other AH systems.
%GRATA could be applied in other domains as well.  What are some examples?

Our stack currently runs on Baxter, but the requirements are not
stringent.  In fact, it would be possible to execute all scanning and
some training for crane grasps on a modified 3D printer. Furthermore,
the parallel electric gripper for Baxter is more difficult to infer
grasps for than the gripper on the PR2.  Even though the PR2 lacks the
IR rangefinder we use, that data could be gathered by a printer
converted from a scanner, and the PR2 could perform its own grasp
training.

It is clear that the system's accuracy and precision would benefit
from the use of more sophisticated imaging equipment such as the
Kinect 2. Better and faster point clouds acquisition would allow the
use of more precise physical models for grasps. It would also open the
way for additional grasp types, such as side and handle grasps.

%Ideas for doing for the paper, otherwise future work: 
%\begin{itemize}
%\item Semantic mapping. 
%\item Detection and manipulation in clutter and occlusion.
%\item Amazon mechanical turk for labels, so we can follow commands and gesture.
%\item Object tracking over time so we can answer questions about what
%  happened to the object.
%\item Object oriented SLAM so we can handle joint localization and mapping.
%\item Semantic mapping of objects over time.  Deciding when to go look
%  again, maintaining history, etc. 
%\item Scaling to lots and lots of objects.
%\item Using the database of lots and lots of objects to do category recognition.
%\item Multiple poses during training (e.g., what happens when you drop
%  the object?)
%\end{itemize}


%% The file named.bst is a bibliography style file for BibTeX 0.99c
{\tiny
\bibliographystyle{plainnat}
\bibliography{main,references}
}
\end{document}

