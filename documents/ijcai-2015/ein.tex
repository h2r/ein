%%%% ijcai15.tex

% These are the instructions for authors for IJCAI-15.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.

\documentclass{article}
% The file ijcai15.sty is the style file for IJCAI-15 (same as ijcai07.sty).
\usepackage{ijcai15}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{color}
%\usepackage{algorithmic}
\usepackage[]{algorithm2e}
\usepackage{hyperref}
\hypersetup{letterpaper,bookmarksopen,bookmarksnumbered,
pdfpagemode=UseOutlines,
colorlinks=true,
linkcolor=blue,
anchorcolor=blue,
citecolor=blue,
filecolor=blue,
menucolor=blue,
urlcolor=blue
}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{latexsym,amssymb,amsmath} % for \Box, \mathbb, split, etc.
\newcommand{\stnote}[1]{\textcolor{blue}{\textbf{ST: #1}}}
\newcommand{\jgonote}[1]{\textcolor{green}{\textbf{JGO: #1}}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{argmax}}\;}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}\;}


% Use the postscript times font!
\usepackage{times}
\newcommand{\algorithmCTxt}{Ordered Confidence Bound\xspace}
\newcommand{\algorithmCFname}{OrderedConfidenceBound}
\newcommand{\algorithmDTxt}{Prior Confidence Bound\xspace}
\newcommand{\algorithmDFname}{PriorConfidenceBound}

% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Bandit-Based Adaptation for Robotic Grasping}


\author{}

\begin{document}

\maketitle

\begin{abstract}
A key aim of current research is to create robots that can reliably
manipulate objects.  However, in many instances, information needed to
infer a successful grasp is latent in the environment and arises from
complicated physical dynamics; for example, a heavy object might need
to be grasped in the middle or else it will twist out of the robot's
gripper.  The contribution of this paper is a formalization of object
picking as an N-armed bandit problem, where each potential grasp point
corresponds to an arm with unknown pick success rate.  This
formalization enables us to apply bandit-based exploration algorithms
to enable a robot to identify the best grasp point by attempting to
pick up the object and tracking its successes and failures.  Because
the number of grasp points is very large, we define a new algorithm
for best arm identification in budgeted bandits that computes
confidence bounds while incorporating prior information, enabling the
robot to quickly find a near optimal arm without pulling all the arms.
We demonstrate that our adaptation step significantly improves
accuracy over a non-adaptive system, enabling a robot to improve
grasping models through experience.


%%   To address this
%% problem, we focus not on {\em category-based} manipulation (pick up
%% any mug) but rather {\em instance-based} manipulation (pick up this
%% mug).  

%% Our framework runs on an unmodified Baxter
%% robot; using our algorithm, a robot can interact with an object for
%% twenty minutes, and then reliably and quickly localize it with vision
%% and pick it up with closed-loop visual servoing. 

%% Instance-based recognition and pose estimation can be highly
%% accurate but require the designer to adapt the system to the specific
%% object being manipulated in large and small ways, ranging from
%% collecting training data, to choosing parameter values, to selecting
%% algorithms for tasks such as image segmentation or bounding box
%% classification.  

\end{abstract}



\section{Introduction}


Robotics will assist us at childcare, help us cook, and provide
service to doctors, nurses, and patients in hospitals. Many of these
tasks require a robot to robustly perceive and manipulate objects in
its environment, yet robust object manipulation remains a challenging
problem.  Systems for general-purpose manipulation are computationally
expensive and do not enjoy high accuracy on novel
objects~\citep{saxena08}.  Transparent or reflective surfaces that are
not visible in IR or RGB make it difficult to infer grasp
points~\citep{lysenkov13}.  A common source of error is the presence
of latent dynamics that emerge from interactions between the object
and the robot's gripper.  For example, a heavy object might fall out
of the robot's gripper unless it grabs it close to the center.


To address these limitations, we propose an approach for enabling a
robot to learn about an object through exploration and adapt its
grasping model accordingly.  We frame the problem of model adaptation
as identifying the best arm for an N-armed bandit
problem~\citep{thompson33} where the robot aims to minimize simple
regret after a finite exploration period~\citep{bubeck09}.  Our robot
can obtain a high-quality reward signal (although sometimes at a
higher cost in time and sensing) by actively collecting additional
information from the environment, and use this reward signal to
adaptively identify grasp points that are likely to succeed.  

Existing algorithms for best arm identification require pulling all
the arms as an initialization step~\citep{mannor04, audibert10,
  chen14}; in the case of identifying grasp points, where each grasp
takes more than 90 seconds and there are more than 1000 potential
arms, this is a prohibitive expense.  To address this problem, we
present a new algorithm, \algorithmDTxt, based on Hoeffding
races~\citep{maron93}. In our approach, the robot pulls arms in an
order determined by a prior, which allows it to try the most promising
arms first. It can then autonomously decide when to stop by bounding
the confidence in the result.  Figure~\ref{fig:ruler} shows the
robot's performance before and after training on a ruler; after
training it grasps the object in the center, improving the success
rate.
\begin{figure}
\subfigure[Before learning.]{\includegraphics[width=0.5\linewidth]{figures/dropping_ruler.png}}%
\subfigure[After learning.]{\includegraphics[width=0.5\linewidth]{figures/holding_ruler.png}}
\caption{Before learning, the robot grasps the ruler near the end, and
  it twists out of the gripper and falls onto the table;
  after learning, the robot grasps near the ruler's center of
  mass.\label{fig:ruler}}
\end{figure}

%% We use slower, more accurate sensing approaches
%% to provide supervision for faster, simpler methods that excel with
%% large amounts of training data.  For example, to perform grasping we
%% use an analytic model to select grasp points, but depending on the
%% object, the best grasp according to the analytic model may not be
%% optimal; the robot can learn better grasps for that object using the
%% analytic model as a prior and actively collecting data for that
%% object.

%%  a view-based model for closed-loop visual picking.  View-based
%% methods for instance detection have many advantages over methods using
%% 3D models, because they directly capture the visual appearance of the
%% object, and are relatively simple and efficient to implement because
%% they operate on low-level features~\citep{hsiao13}.  However these
%% systems require large amounts of training data for robust performance,
%% for example more than 2000 images which must be manually collected and
%% annotated with bounding boxes for the state of the art LINE2D
%% method~\citep{hinterstoisser12}.  Moreover, for manipulation,
%% view-based methods do not propose potential grasps, and autonomous
%% methods for recognizing visual grasps are prone to error. 

%% To address these issues, we present an approach for enabling a robot
%% to train its own view-based model to recognize and manipulate the
%% specific objects it will need to use during collaborations with
%% humans.  Using our algorithm, the robot detects candidate objects for
%% training using a depth sensor, then actively collects view-based
%% visual templates to perform robust instance-based object detection,
%% pose estimation and closed-loop grasping using visual servoing.
%% Because our camera can move with seven degrees of freedom, the robot
%% can collect large quantities of data leading to simple visual models
%% that perform with high accuracy even under occlusion.  Our approach is
%% enabled by three innovations: our end-to-end algorithm for collecting
%% view-based training data with supervision obtained from a
%% higher-reliability depth sensor, which is supported by a simple and
%% robust method for determining candidate grasps using a depth sensor
%% mounted on a seven-degree-of-freedom arm, along with an approach for
%% autonomously and reliably finding object bounding boxes once the
%% object is on a background such as a floor or table.

We evaluated \algorithmDTxt on a Baxter robot, demonstrating that our
adaptation step improves the overall pick success rate from 55\% to
75\% on our test set of 30 household objects, shown in
Figure~\ref{fig:object_glory_shot}.  Moreover, our approach also
enables the robot to learn success probabilities for each object it
encounters; when the robot fails to infer a successful grasp for an
object, it knows this fact, enabling it to take active steps to
recover such as asking for help~\citep{tellex14}.

We first give an overview of our object detection and localization pipeline.
Next we formalize our grasping framework as a bandit problem, where each
arm corresponds to a grasp point on the object.
Section~\ref{sec:evaluation} describes our evaluation in simulation
and on the real robot with $30$ household objects,
Section~\ref{sec:relatedwork} covers related work, and
Section~\ref{sec:conclusion} concludes.


%% Our software is compatible with ROS and the Baxter SDK version 1.0.0,
%% and we intend to release it as free software should our paper be
%% accepted.  As more and more instance-based models are collected, this
%% corpus will form a unique training set for category-based models for
%% detecting and grasping novel objects, since the robot will have a very
%% large number of views of a large set of objects as well as storing
%% depth information and grasping success.

\section{Object Detection and Localization}
Our object detection and pose estimation pipeline uses conventional
computer vision algorithms in a simple software architecture to
achieve a frame rate of about 2Hz for object detection and pose
estimation.  Object classes consist of object instances rather than
pure object categories.  Using instance recognition means we cannot
reliably detect categories, such as ``mugs,'' but the system will be
much better able to detect, localize, and grasp the specific instances for which
it does have models.

Our detection pipeline runs on stock Baxter with one additional
computer, using the arm cameras and IR sensor to localize objects in
the environment.  Our recognition pipeline takes video from the
robot's wrist cameras, proposes a small number of candidate object
bounding boxes in each frame, and classifies each candidate bounding
box as belonging to a previously encountered object class.

When the robot moves to attempt a pick, it uses detected bounding
boxes and visual servoing to move the arm to a position approximately
above the target object. Next it uses image gradients to servo the arm
to a known position and orientation above the object. Because we can
know the arm's position relative to the object, we can reliably
collect statistics about the success rate of grasps at specific points
on the object.


\begin{figure}
  \centering
\includegraphics[height=1in]{figures/ruler_rgb.png}
\includegraphics[height=1in]{figures/ruler_ir.png}
\caption{Automatically acquired RGB image for one of the objects in our
  training set, combined with the IR raster scan.\label{fig:ir}}
\end{figure}

To infer grasp points, we create a depth map of the object using the
robot's IR sensor.  (This process would be much faster with an RGB-D
sensor such as the Kinect, but the advantage of our approach is that
it can be used with a stock Baxter with no additional sensing.)  We
collect many measurements for the depth map by performing an overhead
raster scan of the object. The IR sensor reports at about 30Hz but
gives fairly good localization.  In order to use the good resolution
despite the sparsity of measurement induced by the frequency of the
sensor and the speed of the robot's movement, we employ a Parzen
kernel density estimator to record z measurements at 1mm $(x,y)$
resolution. We then downsample to a $21 \times 21$ grid at 1cm
resolution, which yields a clean depth map and a tractable state space
for grasp inference.  Figure~\ref{fig:ir} shows the original object
together with the results of the IR raster scan.

To infer grasps, we consider 4 gripper orientations at each $(x,y)$
location in the scan for a total of 1764 possible grasps. Our prior
for grasp success is a map $\pi:(x,y,\theta) \rightarrow \mathbb{R}$
generated by convolving the 1cm depth map with 4 linear filters which
correspond to the 4 orientations we consider. The responses are scaled
linearly to be between 0 and 1 inclusive.

This pipeline works well on some objects, but often fails for a
variety of reasons.  For example, our grasp model is not able to 
reliably infer grasps for objects that are not visible in IR, such as
transparent or very dark objects.  Other objects need to be
grasped in particular locations to avoid getting stuck on the gripper
due to the compliance of the object and the dynamics of the robot's
gripper.  The next section defines how we adapt this grasping pipeline
by learning to identify good grasps from experience.

\section{Bandit-based Adaptation}

The formalization we contribute treats grasp learning as an N-armed
bandit problem.  Formally, the agent is given an N-armed bandit, where
each arm pays out $1$ with probability $\mu_i$ and $0$ otherwise.  The
agent's goal is to identify a good arm (with payout $>= k$) with
probability $c$ (e.g., 95\% confidence that this arm is good) as
quickly as possible.  As soon as it has done this, it should
terminate.  The agent is also given a prior $\pi$ on the arms so that
it may make informed decision about which grasps to explore.

Our contributed algorithm, \algorithmDTxt, iteratively chooses the arm
with the highest observed (or prior) success rate but whose
probability of being below $k$ is less than $c$. It then tries that
arm, records the results, and checks to see if its probability of
success is above $k$ or within $[k-\epsilon, k+\epsilon]$ with
probability $c$ (in which case it terminates). (If the latter
condition is not included, an arm with success probability equal to
$k$ will continue to be pulled indefinitely.)  Pseudo-code appears in
Algorithm~\ref{alg:bandit2}.  To compute this probability we need to
estimate the probability that the true payout probability, $\mu$ is
greater than the threshold, $c$ given the observed number of successes
and failures:
\begin{align}
\Pr(\mu_i > c|  S, F)
\end{align}

We can compute this probability using the law of total probability:
\begin{align}
\Pr(\mu_i > c|  S, F) &= 1 - \int_0^k \Pr(\mu_i=\mu | S, F) d\mu\\
\intertext{We assume a beta distribution on $\mu$:}
                      &= \int_k^1 \mu^S (1- \mu) ^F d\mu
\end{align}

This integral is the CDF of the beta distribution, and is called the
regularized incomplete beta function~\citep{olver10}. 


\begin{algorithm}[t]
\SetKwFunction{x}{\algorithmDFname}
\x{$\pi$, $k$, $\delta_{accept}$, $\delta_{reject}$, $maxTries$}

$\mbox{Initialize } S_0 \dots S_n \mbox{ to } \pi(0)...\pi(n)$\\
$\mbox{Initialize } F_0 \dots F_n \mbox{ to } 0$\\
$totalTries \gets 0$\\
\While {$true$}{
  $totalTries \gets totalTries + 1$\\
  $\mbox{Set } M_0 \dots M_n \mbox{ to } \frac{S_0}{S_0 + F_0}...\frac{S_n}{S_n + F_n}$\\

  $j \gets bestValidArm$\tcp*[r]{set j to the arm with $p_{below} < \delta_{reject}$ that has the highest marginal value}

  $r \gets sample(arm_j)$\\
  \uIf {$r = 1$}{
    $S_j  \gets S_j + 1$
  }\Else{
    $F_j  \gets F_j + 1$
  }
  $p_{below} \gets \int_0^k \Pr(\mu_j = \mu | S_j, F_j) d\mu $\\
  $p_{above} \gets \int_k^1 \Pr(\mu_j = \mu | S_j, F_j) d\mu $\\ %1 - p_{below}$\\
  $p_{threshold} \gets \int_{k - \epsilon}^{k + \epsilon} \Pr(\mu_j = \mu | S_j, F_j) d\mu $\\

  \uIf{$p_{above} \ge  \delta_{accept}$}{
    return $j$ \tcp*[r]{accept this arm}
  }\uElseIf{$p_{threshold} \ge \delta_{accept}$}{
    return $j$ \tcp*[r]{accept this arm}
  }\uElseIf{$totalTries > maxTries$}{
    return $maxI$ \tcp*[r]{return the arm with the best marginal value out of those that were tried}
  } \Else{
    pass \tcp*[r]{keep trying}
  }
}

\caption{\algorithmDTxt for Best Arm
  Identification\label{alg:bandit2}}
\end{algorithm}


%% * terminate fast, and know when to terminate
%% * identify the best grasp with high probability (and search longer if you haven't)
%% * providing a prior
%% * providing a bound on mu. 

%To obtain the 3D point cloud maps, we use the IR sensor in
%conjunction with the RGB camera.

%% The IR sensor is a triangulation rangefinder, which means it actively
%% illuminates its target with an infrared LED. If the robot is totally
%% deprived of light, the spot of light projected by the IR LED can be
%% seen in the RGB camera image. The location of the point in the image
%% changes significantly with the range of the point being measured. We
%% record the location at several reference ranges and interpolate
%% between the two closest reference points, allowing us to know the
%% pixel in the RGB image corresponding to the $(x,y,z)$ point being
%% measured. Together, this forms an $(r,g,b,x,y,z)$ measurement for the
%% pointcloud.

\section{Evaluation}
\label{sec:evaluation}

\begin{figure}
\includegraphics[width=1\linewidth]{figures/object_glory_shot.jpg}
\caption{The objects used in our evaluation.\label{fig:object_glory_shot}}
\end{figure}


The aim of our evaluation is to assess the ability of the system to
acquire visual models of objects which are effective for grasping and
object detection.  We first assess our approach in simulation to allow
comparison to baselines, which would be prohibitively time-consuming
to run on the real robot.  Next we describe our robotic evaluation,
which assesses our system's ability to adaptively improve its ability
to grasp objects, end-to-end.

\subsection{Simulation}

We simulate picking performance by creating a sequence of $50$
bandits, where each arm pays out at a rate uniformly sampled between
$0$ and $1$.  For algorithms that incorporate prior knowledge, we
sample a vector of estimates for each $\mu_i$ from a beta distribution
with $\alpha = \beta = 1 + e * \mu_i$ where $e$ controls the entropy
of the sampling distribution.


To compare to a well-known baseline, we assess the performance of
Thompson Sampling~\citep{thompson33} in the fixed budget setting,
although this algorithm minimizes total regret, including regret
during training, rather than simple regret.  Second, we compare to a
Uniform baseline that pulls every arm equally until the budget is
exceeded.  This baseline corresponds to the initialization step in UCB
or the confidence bound algorithms in \citet{chen14}.  If we
implemented the state-of-the-art CLUCB algorithm from \citet{chen14},
it would not have enough pulls to finish this initialization step in
our setting.  Finally, we show the performance of three versions of
\algorithmDTxt, one with an uninformed prior ($e=0$, corresponding to
Hoeffding races~\citep{maron93}), one quite noisy with $e=1$(but still
informative), the other less noisy $e=5$).

We run each experiment for $100$ trials, and report $95\%$ confidence
intervals around the algorithm's simple regret.  For Thompson Sampling
and Uniform, which always use all trials in their budget, we report
performance at each budget level; for \algorithmDTxt, we report the
mean number of trials the algorithm took before halting, also at
$95\%$ confidence intervals.

Results appear in Figure~\ref{fig:simulation_results}.  Thompson
Sampling always uses all trials in its budget and improves performance
as larger budgets are available.  The Uniform method fails to find the
optimal arm because there is not enough information when pulling each
arm once.  All variants of \algorithmDTxt outperform these baselines,
but as more prior information is incorporated, regret decreases.  Even
with a completely uninformed prior, bounding the confidence and
decided when to stop improves performance over Thompson sampling or a
uniform baseline, but the approach realizes significant further
improvement with more prior knowledge.


\begin{figure}
\includegraphics{figures/bestarm.pdf}
\caption{Results comparing our approach to various baselines in simulation.\label{fig:simulation_results}}
\end{figure}


\subsection{Robotic Evaluation}

We implemented our approach on the Baxter robot.  The robot acquired
visual and RGB-D models for 30 objects using our autonomous learning
system.  The objects used in our evaluation appear in
Figure~\ref{fig:object_glory_shot}. We manually verified that the
scans were accurate, and set the following parameters: height above
the object for the IR scan (to approximately 2cm); this height could
be acquired automatically by doing a first coarse IR scan following by
a second IR scan 2cm above the tallest height, but we set it manually
to save time.  Additionally we set the height of the arm for the
initial servo to acquire the object.  After acquiring visual and IR
models for the object at different poses of the arm, the robot
performed the bandit-based adaptation step using
Algorithm~\ref{alg:bandit2}.


After the robot detects an initially successful grab, it shakes the
object vigorously to ensure that it would not fall out during
transport. After releasing the object and moving away, the robot
checks to make sure the object is not stuck in its gripper. If the
object falls out during shaking or does not release properly, the
grasp is recorded as a failure. If the object is stuck, the robot
pauses and requests assistance before proceeding.

Most objects have more than one pose in which they can stand upright
on the table. If the robot knocks over an object, the model taken in
the reference pose is no longer meaningful. Thus, during training, we
monitored the object and returned it to the reference pose whenever
the robot knocked it over. In the future, we aim to incorporate
multiple components in the models which will allow the robot to cope
with objects whose pose can change during training.

Our algorithm used an accept threshold of $0.7$, reject confidence of
$0.95$ and epsilon of $0.2$.  These parameters result in a policy that
rejects a grasp after one failed try, and accepts if the first three
picks are successful.  Different observations of success and failure
will cause the algorithm to try the grasp more to determine the true
probability of success.  

We report the performance of the robot at picking using the learned
height for servoing, but without grasp learning, then the number of
trials used for grasp learning by our algorithm, and finally the
performance at picking using the learned grasp location.  These
results appear in Table~\ref{table:robot_results}.


%% \begin{figure}
%% \includegraphics{figures/policy.pdf}
%% \caption{Policy for an \algorithmCTxt agent given a belief state, defined by an
%%   observed number of successes and failures on that arm for our model
%%   parameters.  Red (-) shows states where the agent rejects the arm
%%   and move to the next one; green (+) shows states where it accepts
%%   the arm and stops learning, and the arrows show movement to the next
%%   belief state based on whether the next trial is successful.\label{fig:policy}}
%% \end{figure}

\begin{table}
\small
\begin{tabular}{cccc}
\toprule
		    & Prior         &  Training     & Marginal\\
\midrule
\multicolumn{4}{c}{$\Delta = 0; training = 3$} \\
\midrule
% too easy
Brush    	    & 10/10         &  3/3         &  10/10\\
Packing Tape        & 9/10          &  3/3         &  9/10 \\
Purple Marker       & 9/10          &  3/3         &  9/10 \\
Red Bowl    	    & 10/10         &  3/3         &  10/10\\
Red Bucket    	    & 5/10          &  3/3         &  5/10 \\
Shoe    	    & 10/10         &  3/3         &  10/10\\
Stamp    	    & 8/10          &  3/3         &  8/10 \\
Whiteout    	    & 10/10         &  3/3         &  10/10\\
Wooden Spoon        & 7/10          &  3/3         &  7/10 \\
% good improvement  
\midrule
\\
\multicolumn{4}{c}{$\Delta >= 2$} \\
\midrule
Big Syringe    	    & 1/10          &  13/50       &  4/10 \\
Blue Salt Shaker    & 6/10          &  5/10        &  8/10 \\
Bottle Top    	    & 0/10          &  5/17        &  7/10 \\
Garlic Press        & 0/10          &  8/50        &  2/10 \\
Gyro Bowl    	    & 0/10          &  5/15        &  3/10 \\
Metal Pitcher       & 6/10          &  7/12        &  10/10\\
Mug    		    & 3/10          &  3/4         &  10/10\\
Round Salt Shaker   & 1/10          &  4/16        &  9/10 \\
Sippy Cup    	    & 0/10          &  6/50        &  4/10 \\
Triangle Block      & 0/10          &  3/13        &  7/10 \\
Vanilla	   	    & 5/10          &  4/5         &  9/10 \\
Wooden Train        & 4/10          &  11/24       &  8/10 \\

\midrule
\\
\multicolumn{4}{c}{$\Delta <= 1$} \\
\midrule
% little to no improvement
Clear Pitcher       & 4/10          &  3/4         &  4/10 \\
Dragon    	    & 8/10          &  5/6         &  7/10 \\
Epipen    	    & 8/10          &  4/5         &  8/10 \\
Helicopter    	    & 2/10          &  8/39        &  3/10 \\
Icosahedron    	    & 7/10          &  7/21        &  8/10 \\
Ruler    	    & 6/10          &  5/12        &  7/10 \\
Syringe    	    & 9/10          &  6/9         &  10/10\\
Toy Egg    	    & 8/10          &  4/5         &  9/10 \\
Yellow Boat    	    & 9/10          &  5/6         &  9/10 \\
\midrule
Total		    & 165/300       &  148/400     & 224/300\\
Rate		    & 0.55          &  0.37        & 0.75\\
\bottomrule
\end{tabular}
\caption{Results from the robotic evaluation of \algorithmDTxt. We tested on 30 objects. We use
$\Delta$ to denote the difference in success rate between the grasp recommended
by the prior (Prior column) and the grasp learned by the system (Marginal
column). The top block contains objects which succeeded on the initial grasp
estimate and therefor did not learn a new grasp. Since the grasp did not change
we report the results from one round of 10 picks as both the prior and marginal
success rate.  The middle block contains objects which spent some time learning
and improved their performance notably. The bottom block contains objects for
which some learning occurred but little or more improvement was seen. A
performance drop after learning was seen on one class:
Dragon.\label{table:robot_results}}
% Curses to the dragon. Curses to Smaug!
% jgo: I double and triple checked the sums.
\end{table}

\subsection{Discussion}

After evaluating, we sorted objects into three blocks, shown in
Table~\ref{table:robot_results}.  We define the quantity $\Delta$ to
be the difference in grasp successes before and after training.  The
first block shows objects where the learning process kept the same
grasp.  For these objects, since the prior and marginal grasps are the
same, we ran that grasp $10$ times and reported it as both the prior
and marginal performance, to save time.  Note that when ten trials are
performed, three consecutive successes do not assure even near perfect
performance.  Using a low accept threshold and confidence means that
sometimes the algorithm will accept a grasp that is actually below the
threshold.  By increasing the confidence, we could obtain higher
certainty that the best grasp was found (including obtaining
PAC~\citep{valiant84} bounds on performance as in \citet{maron93}),
but at the cost of significantly more trails.

The second block in the table shows objects that improved performance
by more than two pick successes.  These objects typically had some
feature that prevented our grasping model from working, for example,
being difficult to view in IR.  For example, the triangular block
failed with the prior grasp because the gripper slid over the sloped
edges and pinched the block out of its grippers.  The robot tried
grasps until it found one that targeted the sides that were parallel
to the grippers, resulting in a flush grasp, significantly improving
accuracy.  For the round salt shaker, the robot first attempted to
grab the round plastic dome, which is infeasible. It tried grasps
until it found one on the handle that worked well.


Objects such as the round salt shaker and the bottle top are on the
edge of tractability for thorough policies such as Thompson
sampling. \algorithmDTxt, on the other hand, rejects arms quickly so
as to make these two objects train in relatively short order while
bringing even more difficult objects such as the sippy cup and big
syringe into the realm of possibility. It would have taken
substantially more time and picks for Thompson sampling to reject the
long list of bad grasps before finding the good ones.

The garlic press is a geometrically simple object but quite heavy
compared to the others. The robot found a few grasps which might have
been good for a lighter object, but it frequently shook the press out
of its grippers when confirming grasp quality.  The big syringe has
some good grasps which are detected well by the prior, but due to its
poor contrast and transparent tip, orientation servoing was imprecise
and the robot was unable to learn well due to poor signal. What
improvement did occur was due to finding a grasp which consistently
deformed the bulb into a grippable shape regardless of the perceived
orientation of the syringe. Similar problems were observed with the
clear pitcher and icosahedron.

%% Some of the objects had several grasps of similar, mediocre quality,
%% which caused the robot to try multiple grasps several times,
%% eventually accepting one of the mediocre grasps by chance.  For
%% example, the helicopter, wooden train, and icosahedron.

Objects that failed to improve, shown in the next segment, fall into
several categories.  For some, performance was already high, so there
was not much room to move.  A common failure mode for poorly
performing objects was failure to accurately determing the position
and orientation through visual servoing.  If the grasp map cannot be
localized accurately, significant noise is introduced because the map
does not correspond to the same physical location on the object at
each trial.  For example, there is only about a 5mm difference
between the width of the dragon and the width of the gripper; objects
such as these would benefit from additional servo iterations to
increase localization precision. If we double the number of iterations
during fine grained servoing we can more reliably pick it, but this
would either introduce another parameter in the system (iterations) or
excessively slow down other objects which are more tolerant to error.



%% The red bucket exhibited interesting behavior. Its gradient profile is
%% rectangular, but at the chosen servo height only two opposing sides
%% are visible in the camera. Due to symmetry, the robot found a good
%% grasp despite the degeneracy in the model. Most of this object's
%% failures were because the object is tall and the robot did not raise
%% its arm up enough before checking if its gripper was empty, which
%% triggered false negatives for grasp release.



\section{Related Work}

\label{sec:relatedwork}


Pick-and-place has been studied since the early days of
robotics~\citep{brooks83, lozano89}.  Forerunning systems relied on
the user to provide models of object and end effector pose for the
algorithm, and simply planned a motion for the arm to grasp.
\citet{bohg13} survey data-driven approaches to grasping.  In their
terminology, our approach is a pipeline for automatically building an
experience database consisting of object models and known good grasps.
Our system initially uses analytic approaches to generate a grasp
hypothesis space which allows it to pick previously unencountered
objects. Our bandit-based method tries new grasps and learns
instance-based distributions for the grasp experience database.  In
this way our system achieves the best of both approaches: models for
grasping unknown objects can be applied; if they fail, the system can
attempt to recover by trying grasps and adapting itself based on that
specific object.

Many  approaches use object recognition systems to estimate pose and
object type, then libraries of grasps either annotated or learned from
data~\citep{saxena08, goldfeder09, morales03,ciocarlie14}.  These
approaches attempt to create systems that can grasp arbitrary objects
based on learned visual features or the object's known 3d
configuration.  Collecting these training sets is an expensive process
and is not accessible to the average user in a non-robotics
setting. If the system does not work for the user's particular
application, there is no easy way for it to adapt or relearn.  Our
approach, instead, enables the robot to autonomously acquire more
information to increase robustness at detecting and manipulating the
specific object that is important to the user at the current moment.
Other approaches focus on object discovery and
manipulation~\citep{lyubova13, kraft10, collet14, schiebener12}.  By
formalizing grasp identification as a bandit problem, we are able to
leverege existing strategies for inferring the best arm.

Many existing approaches use a simulator to assess planned grasp
quality, because assessing grasp quality in simulation is much faster
than trying it on the real robot~\citep{miller04}.  In our approach
it is more expensive to evaluate grasp rates because it
requires actually attempting to pick up the object.  However, by
assessing grasp quality in situ with the end-to-end system, our approach
potentially obtains more accurate grasp statistics 
during training which it can leverage later at inference time.

Crowd-sourced and web robotics have created large databases of objects
and grasps using human supervision on the web~\citep{kent14a, kent14}.
These approaches outperform automatically inferred grasps but still
require humans in the loop.  Our approach can incorporate human
annotations in the form of the prior.  If the annotated grasps work
well, then the robot will quickly converge and stop sampling; if they
are poor grasps, our approach will find better ones.

\citet{nguyen14} learn to manipulate objects such as a light switch or drawer
with a similar self-training approach.  Our work autonomously learns visual
models to detect, pick, and place previously unencountered rigid objects by actively
selecting the best grasp point with a bandit based system, rather than
acquiring models for the manipulation of articulated objects. We rely on
the fixed structure of objects rather than learning how to deal
with structure that can change.

Methods for planning in information space \citep{he08, atanasov13,
  prentice09} have been applied to enable mobile robots to plan
trajectories that avoid failures due to inability to accurately
estimate positions.  \citet{velez11} created a mobile robot that
explores the environment and actively plans paths to acquire views of
objects such as doors.  However it uses a fixed model of the object
being detected rather than updating its model based on the data it has
acquired from the environment.  Our approach is focused instead on
identifyng the best grasp point by actively experimenting with objects
in the world.

Other approaches plan grasps under pose uncertainty~\citep{stulp11} or
collect information from tactile sensors~\citep{hsiao10} using
POMDPs.  \citet{platt11} describe new algorithms for solving POMDPs by
tracking the belief state with a high-fidelity particle filter, but using
a lower-fidelity representation of belief for planning, and tracking
the KL divergence.  \citet{hudson12} used active perception to create
a grasping system capable of carrying out a variety of complex tasks.
Using feedback is critical for good performance, but the model cannot
adapt itself to new objects.  Our approach could be used to improve
any of these systems by defining a space of parameters to optimize,
such as potential grasp points, and then assessing the performance
from experience.

We formalize the problem as an N-armed bandit~\citep{thompson33} where
the robot aims to perform best arm identification~\citep{audibert10,
  chen14}, or alternatively, to minimize simple regret after a finite
exploration period~\citep{bubeck09}.  \citet{audibert10} explored best
arm identification in a fixed budget setting; however a fixed budget
approach does not match our problem, because we would like the robot
to stop sampling as soon as it has improved performance above a
threshold.  We take a fixed confidence approach as in \citet{chen14},
but their fixed confidence algorithm begins by pulling each arm once,
a prohibativly expensive operation on our robot.  Instead our
algorithm estimates confidence that one arm is better than another,
following Hoeffding races~\citep{maron93} but operating in a confidence
threshold setting that incorporates prior information.  By
incorporating prior information, our approach achieves good
performance without being required to pull all the arms.


\section{Conclusion}

\label{sec:conclusion}

%\stnote{First paragraph:  contributions.  What are the things this paper has done to advance the state of the art?}
%\stnote{Next paragraphs: future work, spiraling upward to more and
  %more ambitiuos extensions.}
%Software stack for autonomously acquiring instance-based models of objects for the Baxter Research Robot.
%Ordered Confidence Bound algorithm for solving pure exploration bandits with low constant factors by exploiting prior knowledge.

We presented a formalization of the grasping problem as best arm
identification in an N-armed bandit.  Our bandit problem has 1764
levers, so even if we could pull a lever every 10 seconds it would
take around 5 hours to explore all of the arms for a single object.
To address this problem, we created a new algorithm, \algorithmDTxt,
which explores promising arm first by exploiting prior knowledge,
significantly reducing constant factors. 

Our stack gathers feedback from the environment, which it uses to
learn models to detect, localize, and manipulate previously unseen
objects. In the future, we plan to explore learning parameters for a
wider variety of tasks.  For instance, different objects can be
located and oriented better or worse at different heights. One could
learn which heights work well for which objects.  Likewise, we use
color gradients for localization, but some objects would work better
with other quantities. One could learn the appropriate map to use when
localizing each object, or even further, the map might also depend
upon the robot's current environment.

%Right now, NODE runs on Baxter. We will port NODE to PR2 and other AH systems.
%GRATA could be applied in other domains as well.  What are some examples?

Our stack currently runs on Baxter, but the requirements are not
stringent.  In fact, it would be possible to execute all scanning and
some training for crane grasps on a modified 3D printer. Furthermore,
the parallel electric gripper for Baxter is more difficult to infer
grasps for than the gripper on the PR2.  Even though the PR2 lacks the
IR rangefinder we use, that data could be gathered by a printer
converted from a scanner, and the PR2 could perform its own grasp
training.

It is clear that the system's accuracy and precision would benefit
from the use of more sophisticated imaging equipment such as the
Kinect 2. Better and faster point clouds acquisition would allow the
use of more precise physical models for grasps. It would also open the
way for additional grasp types, such as side and handle grasps.

%Ideas for doing for the paper, otherwise future work: 
%\begin{itemize}
%\item Semantic mapping. 
%\item Detection and manipulation in clutter and occlusion.
%\item Amazon mechanical turk for labels, so we can follow commands and gesture.
%\item Object tracking over time so we can answer questions about what
%  happened to the object.
%\item Object oriented SLAM so we can handle joint localization and mapping.
%\item Semantic mapping of objects over time.  Deciding when to go look
%  again, maintaining history, etc. 
%\item Scaling to lots and lots of objects.
%\item Using the database of lots and lots of objects to do category recognition.
%\item Multiple poses during training (e.g., what happens when you drop
%  the object?)
%\end{itemize}


%% The file named.bst is a bibliography style file for BibTeX 0.99c
{\small
\bibliographystyle{plainnat}
\bibliography{main,references}
}
\end{document}

